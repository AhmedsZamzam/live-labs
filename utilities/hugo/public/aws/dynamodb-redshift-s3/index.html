<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Hugo 0.93.0" />
    <meta name="description" content="">


    <link rel="shortcut icon" href="https://confluentinc.github.io/live-labs/images/favicon.png" type="image/x-icon" />
    <title>Build An End-to-End Streaming Data Pipeline with Confluent Cloud :: Confluent Live Labs</title>

    
    <link href="https://confluentinc.github.io/live-labs/css/nucleus.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/fontawesome-all.min.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/hybrid.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/featherlight.min.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/perfect-scrollbar.min.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/auto-complete.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/atom-one-dark-reasonable.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/theme.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/tabs.css?1646074555" rel="stylesheet">
    <link href="https://confluentinc.github.io/live-labs/css/hugo-theme.css?1646074555" rel="stylesheet">
    
    <link href="https://confluentinc.github.io/live-labs/css/theme-confluent.css?1646074555" rel="stylesheet">
    
    

    <script src="https://confluentinc.github.io/live-labs/js/jquery-3.3.1.min.js?1646074555"></script>

    <style>
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
    </style>
    
  </head>
  <body class="" data-url="https://confluentinc.github.io/live-labs/aws/dynamodb-redshift-s3/">
    <nav id="sidebar" class="">



  <div id="header-wrapper">
    <div id="header">
      <a id="logo" href='https://confluentinc.github.io/live-labs/'>
    
    <svg version="1.1" id="confluent-logo" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
         viewBox="0 0 1475 467.84" style="width:100%; height:100%;" xml:space="preserve">
    <style type="text/css">
        .st0{fill:none;}
        .st1{fill:#FFFFFF;}
    </style>
    <g>
        <rect class="st0" width="1475" height="467.84"/>
    </g>
    <g>
        <g>
            <path class="st1" d="M482.17,246.58h11.01c-4.3,20.54-21.08,34.37-43.5,34.37c-26.32,0-47.53-20.41-47.53-47.39
                s21.21-47.26,47.53-47.26c22.42,0,39.2,13.69,43.5,34.24h-11.01c-3.76-14.9-15.84-24.3-32.49-24.3
                c-20.68,0-36.79,15.98-36.79,37.32s16.11,37.46,36.79,37.46C466.33,271.01,478.41,261.35,482.17,246.58"/>
            <path class="st1" d="M602.72,233.55c0-20.68-16.11-37.32-36.78-37.32c-20.68,0-36.79,16.65-36.79,37.32
                c0,20.68,16.11,37.32,36.79,37.32C586.61,270.88,602.72,254.23,602.72,233.55 M613.46,233.55c0,26.31-21.21,47.26-47.53,47.26
                c-26.32,0-47.53-20.95-47.53-47.26c0-26.32,21.21-47.26,47.53-47.26C592.25,186.29,613.46,207.24,613.46,233.55"/>
        </g>
        <polygon class="st1" points="722.2,187.91 722.2,279.2 713.34,279.2 654.94,207.24 654.94,279.2 644.33,279.2 644.33,187.91 
            652.93,187.91 711.6,260.81 711.6,187.91 	"/>
        <polygon class="st1" points="771,197.71 771,231.27 817.45,231.27 817.45,241.07 771,241.07 771,279.2 760.39,279.2 760.39,187.9 
            823.09,187.9 823.09,197.71 	"/>
        <polygon class="st1" points="920.69,269.4 920.69,279.2 859.73,279.2 859.73,187.91 870.34,187.91 870.34,269.4 	"/>
        <g>
            <path class="st1" d="M1021.77,240.13c0,24.3-15.57,40.68-39.07,40.68c-23.49,0-39.07-16.38-39.07-40.68V187.9h10.61v52.23
                c0,17.86,11.28,30.75,28.46,30.75c17.05,0,28.46-12.89,28.46-30.75V187.9h10.61V240.13z"/>
        </g>
        <polygon class="st1" points="1069.59,197.71 1069.59,228.99 1117.52,228.99 1117.52,238.79 1069.59,238.79 1069.59,269.4 
            1122.89,269.4 1122.89,279.2 1058.98,279.2 1058.98,187.9 1122.89,187.9 1122.89,197.71 	"/>
        <polygon class="st1" points="1232.47,187.91 1232.47,279.2 1223.61,279.2 1165.21,207.24 1165.21,279.2 1154.6,279.2 
            1154.6,187.91 1163.2,187.91 1221.87,260.81 1221.87,187.91 	"/>
        <polygon class="st1" points="1260.63,187.91 1260.63,197.71 1294.19,197.71 1294.19,279.2 1304.8,279.2 1304.8,197.71 
            1338.5,197.71 1338.5,187.91 	"/>
        <g>
            <path class="st1" d="M263.68,230.76c-8.73-0.27-17.47-0.34-26.2-0.39c-0.02-8.74-0.06-17.48-0.29-26.21l-0.42-14.88
                c-0.25-4.96-0.4-9.92-0.75-14.88h-4.2c-0.35,4.96-0.5,9.92-0.75,14.88l-0.42,14.88c-0.11,4.1-0.16,8.2-0.21,12.3
                c-1.61-3.77-3.23-7.54-4.9-11.29l-6.08-13.59c-2.13-4.49-4.17-9.01-6.39-13.46l-3.88,1.61c1.57,4.72,3.34,9.35,5,14.03l5.31,13.91
                c1.47,3.83,2.99,7.64,4.52,11.44c-2.93-2.87-5.87-5.73-8.84-8.55l-10.82-10.23c-3.69-3.33-7.3-6.73-11.05-9.99l-2.97,2.97
                c3.26,3.76,6.66,7.37,9.99,11.05l10.23,10.82c2.82,2.98,5.68,5.91,8.55,8.84c-3.8-1.53-7.61-3.05-11.44-4.52l-13.91-5.31
                c-4.68-1.66-9.32-3.43-14.04-5l-1.61,3.88c4.45,2.22,8.98,4.25,13.46,6.39l13.59,6.08c3.75,1.67,7.51,3.29,11.28,4.9
                c-4.1,0.04-8.2,0.1-12.3,0.21l-14.88,0.42c-4.96,0.25-9.92,0.4-14.88,0.75v4.2c4.96,0.35,9.92,0.5,14.88,0.75l14.88,0.42
                c8.74,0.24,17.48,0.28,26.21,0.3c0.05,8.73,0.12,17.47,0.39,26.2l0.46,14.88c0.27,4.96,0.43,9.92,0.79,14.88h3.81
                c0.36-4.96,0.52-9.92,0.79-14.88l0.46-14.88c0.13-4.2,0.2-8.4,0.26-12.6c1.66,3.86,3.33,7.71,5.06,11.54l6.12,13.57
                c2.15,4.48,4.19,9,6.42,13.45l3.52-1.46c-1.57-4.72-3.31-9.36-4.97-14.05l-5.27-13.92c-1.49-3.93-3.03-7.84-4.58-11.74
                c3.01,2.93,6.03,5.85,9.09,8.73l10.85,10.2c3.7,3.32,7.32,6.71,11.08,9.96l2.7-2.7c-3.25-3.76-6.64-7.39-9.96-11.08l-10.2-10.85
                c-2.88-3.06-5.8-6.08-8.73-9.09c3.9,1.55,7.81,3.1,11.74,4.58l13.93,5.27c4.68,1.65,9.33,3.4,14.05,4.97l1.46-3.52
                c-4.44-2.23-8.97-4.28-13.45-6.42l-13.57-6.12c-3.83-1.73-7.68-3.4-11.54-5.06c4.2-0.06,8.4-0.13,12.6-0.26l14.88-0.46
                c4.96-0.27,9.92-0.43,14.88-0.79v-3.81c-4.96-0.36-9.92-0.52-14.88-0.79L263.68,230.76z"/>
            <path class="st1" d="M233.92,136.5c-53.72,0-97.42,43.7-97.42,97.42c0,53.72,43.7,97.42,97.42,97.42s97.42-43.7,97.42-97.42
                C331.34,180.2,287.64,136.5,233.92,136.5z M233.92,322.23c-48.69,0-88.31-39.61-88.31-88.31c0-48.69,39.61-88.31,88.31-88.31
                c48.7,0,88.31,39.62,88.31,88.31C322.23,282.61,282.62,322.23,233.92,322.23z"/>
        </g>
    </g>
    </svg>
    
</a>
  
    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="search" placeholder="">
    <span data-search-clear=""><i class="fas fa-times"></i></span>
</div>

<script type="text/javascript" src="https://confluentinc.github.io/live-labs/js/lunr.min.js?1646074555"></script>
<script type="text/javascript" src="https://confluentinc.github.io/live-labs/js/auto-complete.js?1646074555"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/confluentinc.github.io\/live-labs\/";
    
</script>
<script type="text/javascript" src="https://confluentinc.github.io/live-labs/js/search.js?1646074555"></script>

    
  </div>
  
    <section id="homelinks">
      <ul>
        <li>
            <a class="padding" href='https://confluentinc.github.io/live-labs/'><i class='fas fa-home'></i> Home</a>
        </li>
      </ul>
    </section>
  

    <div class="highlightable">
    <ul class="topics">

        
          
          




 
  
    
    <li data-nav-id="/gcp/" title="Gcp" class="dd-item
        
        
        
        ">
      <a href="https://confluentinc.github.io/live-labs/gcp/">
          <b>X. </b>Gcp
          
      </a>
      
      
        <ul>
          
          
            
          
          

        
          
            
            




 
  
    
    <li data-nav-id="/gcp/cloudstorage-bigquery-bigtable/" title="CloudStorage Bigquery Bigtable" class="dd-item
        
        
        
        ">
      <a href="https://confluentinc.github.io/live-labs/gcp/cloudstorage-bigquery-bigtable/">
          CloudStorage Bigquery Bigtable
          
      </a>
      
      
    </li>
  
 

            
          
        
        </ul>
      
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/aws/" title="AWS" class="dd-item
        parent
        
        
        ">
      <a href="https://confluentinc.github.io/live-labs/aws/">
          <b>X. </b>AWS
          
      </a>
      
      
        <ul>
          
          
          

        
          
            
            




 
  
    
      <li data-nav-id="/aws/test/" title="Test" class="dd-item ">
        <a href="https://confluentinc.github.io/live-labs/aws/test/">
        Test
        
        </a>
    </li>
     
  
 

            
          
            
            




 
  
    
      <li data-nav-id="/aws/dynamodb-redshift-s3/" title="Build An End-to-End Streaming Data Pipeline with Confluent Cloud" class="dd-item active">
        <a href="https://confluentinc.github.io/live-labs/aws/dynamodb-redshift-s3/">
        Build An End-to-End Streaming Data Pipeline with Confluent Cloud
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
      
    </li>
  
 

          
        
    </ul>

    
    

    
    <section id="footer">
      <p>Built with <a href="https://github.com/matcornic/hugo-theme-learn"><i class="fas fa-heart"></i></a> from <a href="https://getgrav.org">Grav</a> and <a href="https://gohugo.io/">Hugo</a></p>

    </section>
  </div>
</nav>




        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                 
                 
                    
          
          
            
            
          
          
            
            
          
          
            <a href='https://confluentinc.github.io/live-labs/'></a> > <a href='https://confluentinc.github.io/live-labs/aws/'>AWS</a> > Build An End-to-End Streaming Data Pipeline with Confluent Cloud
          
        
          
        
          
        
                 
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li><a href="#agendaagenda"><a href="#agenda">Agenda</a></a></li>
    <li><a href="#architecture-diagramarchitecture-diagram"><a href="#architecture-diagram">Architecture Diagram</a></a></li>
    <li><a href="#fictionair">FictionAir</a></li>
    <li><a href="#fictionmedia">FictionMedia</a></li>
    <li><a href="#prerequisitesprerequisites"><a href="#prerequisites">Prerequisites</a></a>
      <ul>
        <li><a href="#sign-up-for-confluent-cloud-account">Sign up for Confluent Cloud Account</a></li>
        <li><a href="#test-network-connectivity">Test Network Connectivity</a></li>
      </ul>
    </li>
    <li><a href="#sign-up-for-aws-account">Sign up for AWS account</a></li>
    <li><a href="#a-namestep1astep-1-log-into-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 1: Log into Confluent Cloud</a></li>
    <li><a href="#a-namestep2astep-2-create-an-environment-and-cluster"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 2: Create an environment and cluster</a></li>
    <li><a href="#a-namestep3astep-3-create-an-api-key-pair"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 3: Create an API key pair</a></li>
    <li><a href="#a-namestep4astep-4-enable-schema-registery"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 4: Enable Schema Registery</a></li>
    <li><a href="#hands-on-labhandson"><a href="#handson">Hands-on Lab</a></a></li>
    <li><a href="#you-have-successfully-completed-the-prep-work-you-can-stop-at-this-point-and-complete-the-remaining-steps-during-the-live-session"><strong>You have successfully completed the prep work. You can stop at this point and complete the remaining steps during the live session</strong></a></li>
    <li><a href="#a-namestep5astep-1-create-a-ksqldb-application"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 1: Create a ksqlDB application</a></li>
    <li><a href="#a-namestep6astep-2-create-ratings-topic"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 2: Create &ldquo;ratings&rdquo; topic</a></li>
    <li><a href="#a-namestep7astep-3-create-a-datagen-source-connector"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 3: Create a Datagen Source connector</a></li>
    <li><a href="#a-namestep8astep-4-create-customers-topic"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 4: Create customers topic</a></li>
    <li><a href="#a-namestep9astep-5-create-a-mysql-cdc-source-connector"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 5: Create a MySQL CDC Source connector</a></li>
    <li><a href="#a-namestep10astep-6-create-users-topic"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 6: Create &ldquo;users&rdquo; topic</a></li>
    <li><a href="#a-namestep7astep-7-create-a-datagen-source-connector"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 7: Create a Datagen Source connector</a></li>
    <li><a href="#a-namestep11astep-8-create-aws-services"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 8: Create AWS services</a>
      <ul>
        <li><a href="#redshift">Redshift</a></li>
        <li><a href="#s3">S3</a></li>
        <li><a href="#iam-policy-and-user">IAM Policy and User</a></li>
      </ul>
    </li>
    <li><a href="#a-namestep12astep-9-enrich-data-streams-with-ksqldb"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 9: Enrich data streams with ksqlDB</a></li>
    <li><a href="#a-namestep13astep-10-connect-redshift-sink-to-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 10: Connect Redshift sink to Confluent Cloud</a></li>
    <li><a href="#a-namestep14astep-11-connect-s3-sink-to-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 11: Connect S3 sink to Confluent Cloud</a></li>
    <li><a href="#a-namestep15astep-12-connect-dynamodb-sink-to-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 12: Connect DynamoDB sink to Confluent Cloud</a></li>
    <li><a href="#a-namestep16astep-13-clean-up-resources"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 13: Clean up resources</a></li>
    <li><a href="#a-namestep17aconfluent-resources-and-further-testing"><!-- raw HTML omitted --><!-- raw HTML omitted -->Confluent Resources and Further Testing</a></li>
  </ul>
</nav>
    </div>
</div>

                
              </div>
            </div>
            
        <div id="head-tags">
        
        </div>
        
        <div id="body-inner">
          
            <h1>
              
              Build An End-to-End Streaming Data Pipeline with Confluent Cloud
            </h1>
          

        


<hr>
<h2 id="agendaagenda"><a href="#agenda">Agenda</a></h2>
<ol>
<li><a href="#step1">Log into Confluent Cloud</a></li>
<li><a href="#step2">Create an environment and cluster</a></li>
<li><a href="#step3">Create an API key pair</a></li>
<li><a href="#step4">Enable Schema registery</a></li>
<li><a href="#step5">Create a ksqlDB application</a></li>
<li><a href="#step6">Create &ldquo;ratings&rdquo; topic</a></li>
<li><a href="#step7">Create a Datagen Source connector</a></li>
<li><a href="#step8">Create customers topic</a></li>
<li><a href="#step9">Create a MySQL CDC Source connector</a></li>
<li><a href="#step10">Create &ldquo;users&rdquo; topic</a></li>
<li><a href="#step11">Create a Datagen Source connector</a></li>
<li><a href="#step12">Create AWS services</a></li>
<li><a href="#step13">Enrich data streams with ksqlDB</a></li>
<li><a href="#step14">Connect Redshift sink to Confluent Cloud</a></li>
<li><a href="#step15">Connect S3 sink to Confluent Cloud</a></li>
<li><a href="#step16">Connect DynamoDB sink to Confluent Cloud</a></li>
<li><a href="#step17">Clean up resources</a></li>
</ol>
<hr>
<h2 id="architecture-diagramarchitecture-diagram"><a href="#architecture-diagram">Architecture Diagram</a></h2>
<p>This lab will be utilizing two fully-managed source connectors (Datagen and MySQL CDC) and three fully-managed sink connectors (AWS Redshift, S3, and DynamoDB).</p>
<h2 id="fictionair">FictionAir</h2>
<!-- raw HTML omitted -->
<h2 id="fictionmedia">FictionMedia</h2>
<!-- raw HTML omitted -->
<hr>
<h2 id="prerequisitesprerequisites"><a href="#prerequisites">Prerequisites</a></h2>
<h3 id="sign-up-for-confluent-cloud-account">Sign up for Confluent Cloud Account</h3>
<ol>
<li>
<p>Sign up for a Confluent Cloud account <a href="https://www.confluent.io/get-started/">here</a>.</p>
</li>
<li>
<p>Once you have signed up and logged in, click on the menu icon at the upper right hand corner, click on “Billing &amp; payment”, then enter payment details under “Payment details &amp; contacts”.</p>
</li>
</ol>
<blockquote>
<p><strong>Note:</strong> You will create resources during this lab that will incur costs. When you sign up for a Confluent Cloud account, you will get free credits to use in Confluent Cloud. This will cover the cost of resources created during the lab. More details on the specifics can be found <a href="https://www.confluent.io/get-started/">here</a>.</p>
</blockquote>
<h3 id="test-network-connectivity">Test Network Connectivity</h3>
<ol>
<li>Ports <code>443</code> and <code>9092</code> need to be open to the public internet for outbound traffic. To check, try accessing the following from your web browser:</li>
</ol>
<ul>
<li>portquiz.net:443</li>
<li>portquiz.net:9092</li>
</ul>
<h2 id="sign-up-for-aws-account">Sign up for AWS account</h2>
<ol>
<li>In order to complete this lab, you need to have an AWS account that has root level permissions. Sign up for an AWS account <a href="https://aws.amazon.com/account/">here</a>.</li>
</ol>
<hr>
<h2 id="a-namestep1astep-1-log-into-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 1: Log into Confluent Cloud</h2>
<ol>
<li>First, access Confluent Cloud sign-in by navigating <a href="https://confluent.cloud">here</a>.</li>
<li>When provided with the <em>username</em> and <em>password</em> prompts, fill in your credentials.
<blockquote>
<p><strong>Note:</strong> If you&rsquo;re logging in for the first time you will see a wizard that will walk you through the some tutorials. Minimize this as you will walk through these steps in this guide.</p>
</blockquote>
</li>
</ol>
<hr>
<h2 id="a-namestep2astep-2-create-an-environment-and-cluster"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 2: Create an environment and cluster</h2>
<p>An environment contains Confluent clusters and its deployed components such as Connect, ksqlDB, and Schema Registry. You have the ability to create different environments based on your company&rsquo;s requirements. Confluent has seen companies use environments to separate Development/Testing, Pre-Production, and Production clusters.</p>
<ol>
<li>
<p>Click <strong>+ Add environment</strong>.</p>
<blockquote>
<p><strong>Note:</strong> There is a <em>default</em> environment ready in your account upon account creation. You can use this <em>default</em> environment for the purpose of this lab if you do not wish to create an additional environment.</p>
</blockquote>
<ul>
<li>Specify a meaningful <code>name</code> for your environment and then click <strong>Create</strong>.
<blockquote>
<p><strong>Note:</strong> It will take a few minutes to assign the resources to make this new environment available for use.</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>Now that you have an environment, let&rsquo;s create a cluster. Select <strong>Create Cluster</strong>.</p>
<blockquote>
<p><strong>Note</strong>: Confluent Cloud clusters are available in 3 types: <strong>Basic</strong>, <strong>Standard</strong>, and <strong>Dedicated</strong>. Basic is intended for development use cases so you should use that for this lab. Basic clusters only support single zone availability. Standard and Dedicated clusters are intended for production use and support Multi-zone deployments. If you’re interested in learning more about the different types of clusters and their associated features and limits, refer to this <a href="https://docs.confluent.io/current/cloud/clusters/cluster-types.html">documentation</a>.</p>
</blockquote>
<ul>
<li>
<p>Choose the <strong>Basic</strong> cluster type.</p>
</li>
<li>
<p>Click <strong>Begin Configuration</strong>.</p>
</li>
<li>
<p>Choose <strong>AWS</strong> as your Cloud Provider and your preferred Region.</p>
<blockquote>
<p><strong>Note:</strong> AWS account with root permissions is required as your Cloud Provider since you will be utilizing Redshift, S3, and DynamoDB in this lab. We recommend you choose Oregon (west2) as the region.</p>
</blockquote>
</li>
<li>
<p>Specify a meaningful <strong>Cluster Name</strong> and then review the associated <em>Configuration &amp; Cost</em>, <em>Usage Limits</em>, and <em>Uptime SLA</em> before clicking <strong>Launch Cluster</strong>.</p>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="a-namestep3astep-3-create-an-api-key-pair"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 3: Create an API key pair</h2>
<ol>
<li>Select API keys on the navigation menu.</li>
<li>If this is your first API key within your cluster, click <strong>Create key</strong>. If you have set up API keys in your cluster in the past and already have an existing API key, click <strong>+ Add key</strong>.</li>
<li>Select <strong>Global Access</strong>, then click Next.</li>
<li>Save your API key and secret - you will need these during the lab.</li>
<li>After creating and saving the API key, you will see this API key in the Confluent Cloud UI in the API keys tab. If you don’t see the API key populate right away, refresh the browser.</li>
</ol>
<hr>
<h2 id="a-namestep4astep-4-enable-schema-registery"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 4: Enable Schema Registery</h2>
<ol>
<li>On the navigation menu, select <strong>Schema Registery</strong>.</li>
<li>Click <strong>Set up on my own</strong>.</li>
<li>Choose <strong>AWS</strong> as the cloud provider and a supported <strong>Region</strong></li>
<li>Click on <strong>Enable Schema Registry</strong>.</li>
</ol>
<hr>
<h2 id="hands-on-labhandson"><a href="#handson">Hands-on Lab</a></h2>
<h2 id="you-have-successfully-completed-the-prep-work-you-can-stop-at-this-point-and-complete-the-remaining-steps-during-the-live-session"><strong>You have successfully completed the prep work. You can stop at this point and complete the remaining steps during the live session</strong></h2>
<h2 id="a-namestep5astep-1-create-a-ksqldb-application"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 1: Create a ksqlDB application</h2>
<blockquote>
<p>At Confluent we developed ksqlDB, the database purpose-built for stream processing applications. ksqlDB is built on top of Kafka Streams, powerful Java library for enriching, transforming, and processing real-time streams of data. Having Kafka Streams at its core means ksqlDB is built on well-designed and easily understood layers of abstractions. So now, beginners and experts alike can easily unlock and fully leverage the power of Kafka in a fun and accessible way.</p>
</blockquote>
<ol>
<li>On the navigation menu, select <strong>ksqlDB</strong>.</li>
<li>Click on <strong>Create cluster myself</strong>.</li>
<li>Choose <strong>Global access</strong> for the access level and hit <strong>Continue</strong>.</li>
<li>Pick a name or leave the name as is.</li>
<li>Select <strong>1</strong> as the cluster size.</li>
<li>Hit <strong>Launch Cluster!</strong>.</li>
</ol>
<hr>
<h2 id="a-namestep6astep-2-create-ratings-topic"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 2: Create &ldquo;ratings&rdquo; topic</h2>
<ol>
<li>On the navigation menu, select <strong>Topics</strong>.</li>
</ol>
<blockquote>
<p>Click <strong>Create topic on my own</strong> or if you already created a topic, click on the <strong>+ Add topic</strong> button on the top right side of the table.</p>
</blockquote>
<ol start="2">
<li>Type <strong>ratings</strong> as the Topic name and hit <strong>Create with defaults</strong>.</li>
</ol>
<hr>
<h2 id="a-namestep7astep-3-create-a-datagen-source-connector"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 3: Create a Datagen Source connector</h2>
<blockquote>
<p>Confluent offers 120+ pre-built <a href="https://www.confluent.io/product/confluent-connectors/">connectors</a>, enabling you to modernize your entire data architecture even faster. These connectors also provide you peace-of-mind with enterprise-grade security, reliability, compatibility, and support.</p>
</blockquote>
<ol>
<li>On the navigation menu, select <strong>Data Integration</strong> and then <strong>Connectors</strong> and <strong>+ Add connector</strong>.</li>
<li>In the search bar search for <strong>Datagen</strong> and select the <strong>Datagen Source</strong> which is a fully-managed connector that we will use to generate sample data with it.</li>
<li>Use the following parameters to configure your connector</li>
</ol>
<pre tabindex="0"><code>{
  &#34;name&#34;: &#34;DatagenSourceConnector_0&#34;,
  &#34;config&#34;: {
    &#34;connector.class&#34;: &#34;DatagenSource&#34;,
    &#34;name&#34;: &#34;DatagenSourceConnector_0&#34;,
    &#34;kafka.auth.mode&#34;: &#34;KAFKA_API_KEY&#34;,
    &#34;kafka.api.key&#34;: &#34;&lt;add_your_api_key&gt;&#34;,
    &#34;kafka.api.secret&#34;: &#34;&lt;add_your_api_secret_key&gt;&#34;,
    &#34;kafka.topic&#34;: &#34;ratings&#34;,
    &#34;output.data.format&#34;: &#34;AVRO&#34;,
    &#34;quickstart&#34;: &#34;RATINGS&#34;,
    &#34;tasks.max&#34;: &#34;1&#34;
  }
}
</code></pre><hr>
<h2 id="a-namestep8astep-4-create-customers-topic"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 4: Create customers topic</h2>
<ol>
<li>On the navigation menu, select <strong>Topics</strong>.</li>
</ol>
<blockquote>
<p>Click <strong>Create topic on my own</strong> or if you already created a topic, click on the <strong>+ Add topic</strong> button on the top right side of the table.</p>
</blockquote>
<ol start="2">
<li>Type <strong>mysql.demo.CUSTOMERS_INFO</strong> as the Topic name. The name of the topic is crucial so make sure you use the exact name and capitalization.</li>
<li>Click on <strong>Show advanced settings</strong> and under <strong>Storage → Cleanup policy → Compact</strong> and <strong>Retention time → Indefinite</strong> and then click on <strong>Create</strong>.</li>
</ol>
<hr>
<h2 id="a-namestep9astep-5-create-a-mysql-cdc-source-connector"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 5: Create a MySQL CDC Source connector</h2>
<ol>
<li>On the navigation menu, select <strong>Data Integration</strong> and then <strong>Connectors</strong> and <strong>+ Add connector</strong>.</li>
<li>In the search bar search for <strong>MySQL CDC</strong> and select the <strong>MySQL CDC Source</strong> which is a fully-managed source connector.</li>
<li>Use the following parameters to configure your connector</li>
</ol>
<pre tabindex="0"><code>{
  &#34;name&#34;: &#34;MySqlCdcSourceConnector_0&#34;,
  &#34;config&#34;: {
    &#34;connector.class&#34;: &#34;MySqlCdcSource&#34;,
    &#34;name&#34;: &#34;MySqlCdcSourceConnector_0&#34;,
    &#34;kafka.auth.mode&#34;: &#34;KAFKA_API_KEY&#34;,
    &#34;kafka.api.key&#34;: &#34;&lt;add_your_api_key&gt;&#34;,
    &#34;kafka.api.secret&#34;: &#34;&lt;add_your_api_secret_key&gt;&#34;,
    &#34;database.hostname&#34;: &#34;&lt;will_be_given_during_lab&gt;&#34;,
    &#34;database.port&#34;: &#34;3306&#34;,
    &#34;database.user&#34;: &#34;&lt;will_be_given_during_lab&gt;&#34;,
    &#34;database.password&#34;: &#34;&lt;will_be_given_during_lab&gt;&#34;,
    &#34;database.server.name&#34;: &#34;mysql&#34;,
    &#34;database.ssl.mode&#34;: &#34;preferred&#34;,
    &#34;snapshot.mode&#34;: &#34;when_needed&#34;,
    &#34;output.data.format&#34;: &#34;AVRO&#34;,
    &#34;after.state.only&#34;: &#34;true&#34;,
    &#34;tasks.max&#34;: &#34;1&#34;
  }
}
</code></pre><hr>
<h2 id="a-namestep10astep-6-create-users-topic"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 6: Create &ldquo;users&rdquo; topic</h2>
<ol>
<li>On the navigation menu, select <strong>Topics</strong>.</li>
</ol>
<blockquote>
<p>Click <strong>Create topic on my own</strong> or if you already created a topic, click on the <strong>+ Add topic</strong> button on the top right side of the table.</p>
</blockquote>
<ol start="2">
<li>Type <strong>users</strong> as the Topic name and hit <strong>Create with defaults</strong>.</li>
</ol>
<hr>
<h2 id="a-namestep7astep-7-create-a-datagen-source-connector"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 7: Create a Datagen Source connector</h2>
<ol>
<li>On the navigation menu, select <strong>Data Integration</strong> and then <strong>Connectors</strong> and <strong>+ Add connector</strong>.</li>
<li>In the search bar search for <strong>Datagen</strong> and select the <strong>Datagen Source</strong> which is a fully-managed connector.</li>
<li>Use the following parameters to configure your connector</li>
</ol>
<pre tabindex="0"><code>{
  &#34;name&#34;: &#34;DatagenSourceConnector_1&#34;,
  &#34;config&#34;: {
    &#34;connector.class&#34;: &#34;DatagenSource&#34;,
    &#34;name&#34;: &#34;DatagenSourceConnector_0&#34;,
    &#34;kafka.auth.mode&#34;: &#34;KAFKA_API_KEY&#34;,
    &#34;kafka.api.key&#34;: &#34;&lt;add_your_api_key&gt;&#34;,
    &#34;kafka.api.secret&#34;: &#34;&lt;add_your_api_secret_key&gt;&#34;,
    &#34;kafka.topic&#34;: &#34;users&#34;,
    &#34;output.data.format&#34;: &#34;AVRO&#34;,
    &#34;quickstart&#34;: &#34;USERS&#34;,
    &#34;tasks.max&#34;: &#34;1&#34;
  }
}
</code></pre><hr>
<h2 id="a-namestep11astep-8-create-aws-services"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 8: Create AWS services</h2>
<ol>
<li>Navigate to <a href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a> and log into your account.</li>
</ol>
<blockquote>
<p>Note: you will need root level permissions in order to complete this lab.</p>
</blockquote>
<h3 id="redshift">Redshift</h3>
<ol>
<li>Create a Redshift cluster and save the <code>Admin user name</code> and <code>Admin user password</code> since you will need it in later steps.</li>
</ol>
<blockquote>
<p>The Redshift cluster has to be in same same region as your Confluent Cloud cluster.</p>
</blockquote>
<ol start="2">
<li>Under <strong>Additional configuration</strong> disable <strong>defaults</strong>.</li>
<li>Make the cluster publicly accessible under <strong>Network and security → Publicly accessible → Enable</strong>.</li>
<li>Using the search bar navigate to <strong>VPC -&gt; Security Groups -&gt; Inbound Rules</strong> and add two new rules for TCP protocol</li>
</ol>
<pre tabindex="0"><code>Type: Redshift
Port range: 5430 
Source: 0.0.0.0/0

Type: Redshift
Port range: 5430 
Source: ::/0
</code></pre><ol start="5">
<li>Navigate back to your Redshift cluster and reboot it to ensure the right security policies are applied.</li>
<li>Once the cluster is in <strong>Available</strong> state use the left handside menu and open <code>Query Editor v2</code> to create a database and a user and give the appropriate permissions.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">DATABASE</span> <span style="color:#f92672">&lt;</span>DB_NAME<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">USER</span> <span style="color:#f92672">&lt;</span>DB_USER<span style="color:#f92672">&gt;</span> PASSWORD <span style="color:#e6db74">&#39;&lt;DB_PASSWORD&gt;&#39;</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">GRANT</span> <span style="color:#66d9ef">USAGE</span> <span style="color:#66d9ef">ON</span> <span style="color:#66d9ef">SCHEMA</span> <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">TO</span> <span style="color:#f92672">&lt;</span>DB_USER<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">GRANT</span> <span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">ON</span> <span style="color:#66d9ef">SCHEMA</span> <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">TO</span> <span style="color:#f92672">&lt;</span>DB_USER<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">GRANT</span> <span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">ON</span> <span style="color:#66d9ef">ALL</span> TABLES <span style="color:#66d9ef">IN</span> <span style="color:#66d9ef">SCHEMA</span> <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">TO</span> <span style="color:#f92672">&lt;</span>DB_USER<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">GRANT</span> <span style="color:#66d9ef">ALL</span> <span style="color:#66d9ef">ON</span> <span style="color:#66d9ef">SCHEMA</span> <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">TO</span> <span style="color:#f92672">&lt;</span>DB_USER<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">GRANT</span> <span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">ON</span> <span style="color:#66d9ef">DATABASE</span> <span style="color:#f92672">&lt;</span>DB_NAME<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">TO</span> <span style="color:#f92672">&lt;</span>DB_USER<span style="color:#f92672">&gt;</span>;
</span></span></code></pre></div><blockquote>
<p>For detailed instructions refer to our <a href="https://docs.confluent.io/cloud/current/connectors/cc-amazon-redshift-sink.html">documentation</a></p>
</blockquote>
<h3 id="s3">S3</h3>
<ol>
<li>Create an S3 bucket and name it <code>confluent-bucket-demo</code>.</li>
</ol>
<blockquote>
<p>The S3 has to be in same same region as your Confluent Cloud cluster.</p>
</blockquote>
<h3 id="iam-policy-and-user">IAM Policy and User</h3>
<ol>
<li>Create an <strong>IAM Policy</strong> with the following configuration and name it <code>confluent-s3-demo-policy</code>.</li>
</ol>
<pre tabindex="0"><code>{
   &#34;Version&#34;:&#34;2012-10-17&#34;,
   &#34;Statement&#34;:[
      {
         &#34;Effect&#34;:&#34;Allow&#34;,
         &#34;Action&#34;:[
            &#34;s3:ListAllMyBuckets&#34;
         ],
         &#34;Resource&#34;:&#34;arn:aws:s3:::*&#34;
      },
      {
         &#34;Effect&#34;:&#34;Allow&#34;,
         &#34;Action&#34;:[
            &#34;s3:ListBucket&#34;,
            &#34;s3:GetBucketLocation&#34;
         ],
         &#34;Resource&#34;:&#34;arn:aws:s3:::confluent-bucket-demo&#34;
      },
      {
         &#34;Effect&#34;:&#34;Allow&#34;,
         &#34;Action&#34;:[
            &#34;s3:PutObject&#34;,
            &#34;s3:GetObject&#34;,
            &#34;s3:AbortMultipartUpload&#34;,
            &#34;s3:ListMultipartUploadParts&#34;,
            &#34;s3:ListBucketMultipartUploads&#34;

         ],
         &#34;Resource&#34;: [
            &#34;arn:aws:s3:::confluent-bucket-demo&#34;,
            &#34;arn:aws:s3:::confluent-bucket-demo/*&#34;
          ]
      }
   ]
}
</code></pre><ol start="2">
<li>
<p>Create an <strong>IAM User</strong> and name it <code>confluent-s3-demo-user</code>. and attach the above policy to it.</p>
</li>
<li>
<p>Attach <code>confluent-s3-demo-policy</code> policy to <code>confluent-s3-demo-user</code> user.</p>
</li>
<li>
<p>Create a key pair for <code>confluent-s3-demo-user</code> user and download the file to use it in later steps.</p>
</li>
</ol>
<blockquote>
<p>For detailed instructions refer to our <a href="https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html">documentation</a></p>
</blockquote>
<ol start="5">
<li>Create a new <strong>IAM Policy</strong> with the following configurations and name it <code>confluent-dynamodb-demo-policy</code>.</li>
</ol>
<pre tabindex="0"><code>{
    &#34;Version&#34;: &#34;2012-10-17&#34;,
    &#34;Statement&#34;: [
        {
            &#34;Sid&#34;: &#34;&lt;optional-identifier&gt;&#34;,
            &#34;Effect&#34;: &#34;Allow&#34;,
            &#34;Action&#34;: [
                &#34;dynamodb:CreateTable&#34;,
                &#34;dynamodb:BatchWriteItem&#34;,
                &#34;dynamodb:Scan&#34;,
                &#34;dynamodb:DescribeTable&#34;
            ],
            &#34;Resource&#34;: &#34;*&#34;
        }
    ]
}
</code></pre><ol start="6">
<li>
<p>Create another <strong>IAM User</strong> and name it <code>confluent-dynamodb-demo-user</code>. This one will be used for DynamoDB.</p>
</li>
<li>
<p>Attach <code>confluent-dynamodb-demo-policy</code> policy to <code>confluent-dynamodb-demo-user</code> user.</p>
</li>
<li>
<p>Create a key pair for <code>confluent-dynamodb-demo-user</code> user and download the file to use it in later steps.</p>
</li>
</ol>
<blockquote>
<p>For detailed instructions refer to our <a href="https://docs.confluent.io/cloud/current/connectors/cc-amazon-dynamo-db-sink.html">documentation</a></p>
</blockquote>
<hr>
<h2 id="a-namestep12astep-9-enrich-data-streams-with-ksqldb"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 9: Enrich data streams with ksqlDB</h2>
<p>Now that you have data flowing through Confluent, you can now easily build stream processing applications using ksqlDB. You are able to continuously transform, enrich, join, and aggregate your data using simple SQL syntax. You can gain value from your data directly from Confluent in real-time. Also, ksqlDB is a fully managed service within Confluent Cloud with a 99.9% uptime SLA. You can now focus on developing services and building your data pipeline while letting Confluent manage your resources for you.</p>
<p>With ksqlDB, you have the ability to leverage streams and tables from your topics in Confluent. A stream in ksqlDB is a topic with a schema and it records the history of what has happened in the world as a sequence of events.</p>
<ol>
<li>Navigate to confluent.cloud</li>
<li>Use the left handside menu and go to the ksqlDB application you created at the beginning of the lab.</li>
</ol>
<blockquote>
<p>You can interact with ksqlDB through the Editor. You can create a stream by using the CREATE STREAM statement and a table using the CREATE TABLE statement. If you’re interested in learning more about ksqlDB and the differences between streams and tables, I recommend reading these two blogs <a href="https://www.confluent.io/blog/kafka-streams-tables-part-3-event-processing-fundamentals/">here</a> and <a href="https://www.confluent.io/blog/how-real-time-stream-processing-works-with-ksqldb/">here</a> or watch ksqlDB 101 course on Confluent Developer <a href="https://developer.confluent.io/learn-kafka/ksqldb/intro/">webiste</a>.</p>
</blockquote>
<p>To write streaming queries against topics, you will need to register the topics with ksqlDB as a stream and/or table.</p>
<ol start="3">
<li>Create a ksqlDB stream from <code>ratings</code> topic.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> STREAM RATINGS_OG <span style="color:#66d9ef">WITH</span> (KAFKA_TOPIC<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ratings&#39;</span>, VALUE_FORMAT<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;AVRO&#39;</span>);
</span></span></code></pre></div><ol start="4">
<li>Change <strong>auto.offset.reset</strong> to <strong>Earliest</strong> and see what&rsquo;s inside the <code>RATINGS_OG</code> stream by running the following query.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span> <span style="color:#66d9ef">FROM</span> RATINGS_OG EMIT CHANGES;
</span></span></code></pre></div><ol start="5">
<li>Create a new stream that doesn&rsquo;t include messages from <code>test</code> channel.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> STREAM RATINGS_LIVE <span style="color:#66d9ef">AS</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">FROM</span> RATINGS_OG
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">WHERE</span> LCASE(CHANNEL) <span style="color:#66d9ef">NOT</span> <span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;%test%&#39;</span>
</span></span><span style="display:flex;"><span>    EMIT CHANGES;
</span></span></code></pre></div><ol start="6">
<li>See what&rsquo;s inside <code>RATINGS_LIVE</code> stream by running the following query.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span> <span style="color:#66d9ef">FROM</span> RATINGS_LIVE EMIT CHANGES;
</span></span></code></pre></div><ol start="7">
<li>
<p>Stop the running query by clicking on <strong>Stop</strong>.</p>
</li>
<li>
<p>Create a stream from customers topic.</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> STREAM CUSTOMERS_INFORMATION
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">WITH</span> (KAFKA_TOPIC <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mysql.demo.CUSTOMERS_INFO&#39;</span>,
</span></span><span style="display:flex;"><span>      KEY_FORMAT  <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;JSON&#39;</span>,
</span></span><span style="display:flex;"><span>      VALUE_FORMAT<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;AVRO&#39;</span>);
</span></span></code></pre></div><ol start="9">
<li>Create <code>customers</code> table based on <code>customers_information</code> stream you just created.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> CUSTOMERS <span style="color:#66d9ef">WITH</span> (FORMAT<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;AVRO&#39;</span>) <span style="color:#66d9ef">AS</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">SELECT</span> id                            <span style="color:#66d9ef">AS</span> customer_id,
</span></span><span style="display:flex;"><span>           LATEST_BY_OFFSET(first_name)  <span style="color:#66d9ef">AS</span> first_name,
</span></span><span style="display:flex;"><span>           LATEST_BY_OFFSET(last_name)   <span style="color:#66d9ef">AS</span> last_name,
</span></span><span style="display:flex;"><span>           LATEST_BY_OFFSET(dob)         <span style="color:#66d9ef">AS</span> dob,
</span></span><span style="display:flex;"><span>           LATEST_BY_OFFSET(email)       <span style="color:#66d9ef">AS</span> email,
</span></span><span style="display:flex;"><span>           LATEST_BY_OFFSET(gender)      <span style="color:#66d9ef">AS</span> gender,
</span></span><span style="display:flex;"><span>           LATEST_BY_OFFSET(club_status) <span style="color:#66d9ef">AS</span> club_status
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">FROM</span>    CUSTOMERS_INFORMATION 
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> id;
</span></span></code></pre></div><ol start="10">
<li>Check to see what&rsquo;s inside the <code>customers</code> table by running the following query.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span> <span style="color:#66d9ef">FROM</span> CUSTOMERS;
</span></span></code></pre></div><ol start="11">
<li>
<p>Now that we have a stream of ratings data and customer information, we can perform a join query to enrich our data stream.</p>
</li>
<li>
<p>Create a new stream by running the following statement.</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> STREAM RATINGS_WITH_CUSTOMER_DATA <span style="color:#66d9ef">WITH</span> (KAFKA_TOPIC<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ratings-enriched&#39;</span>) <span style="color:#66d9ef">AS</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">C</span>.CUSTOMER_ID,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">C</span>.FIRST_NAME <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39; &#39;</span> <span style="color:#f92672">+</span> <span style="color:#66d9ef">C</span>.LAST_NAME <span style="color:#66d9ef">AS</span> FULL_NAME,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">C</span>.DOB,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">C</span>.GENDER,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">C</span>.CLUB_STATUS,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">C</span>.EMAIL,
</span></span><span style="display:flex;"><span>        R.RATING_ID,
</span></span><span style="display:flex;"><span>        R.MESSAGE,
</span></span><span style="display:flex;"><span>        R.STARS,
</span></span><span style="display:flex;"><span>        R.CHANNEL,
</span></span><span style="display:flex;"><span>        TIMESTAMPTOSTRING(R.ROWTIME,<span style="color:#e6db74">&#39;yyyy-MM-dd&#39;&#39;T&#39;&#39;HH:mm:ss.SSSZ&#39;</span>) <span style="color:#66d9ef">AS</span> RATING_TS
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">FROM</span> RATINGS_LIVE R
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">INNER</span> <span style="color:#66d9ef">JOIN</span> CUSTOMERS <span style="color:#66d9ef">C</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">ON</span> R.USER_ID <span style="color:#f92672">=</span> <span style="color:#66d9ef">C</span>.CUSTOMER_ID
</span></span><span style="display:flex;"><span>EMIT CHANGES;
</span></span></code></pre></div><ol start="13">
<li>Change the <strong>auto.offset.reset</strong> to <strong>Earliest</strong> and see what&rsquo;s inside the newly created stream by running the following command.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-SQL" data-lang="SQL"><span style="display:flex;"><span><span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span> <span style="color:#66d9ef">FROM</span> RATINGS_WITH_CUSTOMER_DATA EMIT CHANGES;
</span></span></code></pre></div><ol start="14">
<li>Stop the running query by clicking on <strong>Stop</strong>.</li>
</ol>
<hr>
<h2 id="a-namestep13astep-10-connect-redshift-sink-to-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 10: Connect Redshift sink to Confluent Cloud</h2>
<ol>
<li>The next step is to sink data from Confluent Cloud into Redshift using the fully-managed Redshift Sink connector. The connector will continuosly run and send real time data into Redshift.</li>
<li>First, you will create the connector that will automatically create a Redshift table and populate that table with the data from the <strong>ratings-enriched</strong> topic within Confluent Cloud. From the Confluent Cloud UI, click on the <strong>Data Integration</strong> tab on the navigation menu and select <strong>+Add connector</strong>. Search and click on the Redshift Sink icon.</li>
<li>Enter the following configuration details. The remaining fields can be left blank.</li>
</ol>
<pre tabindex="0"><code>{
  &#34;name&#34;: &#34;RedshiftSinkConnector_0&#34;,
  &#34;config&#34;: {
    &#34;connector.class&#34;: &#34;RedshiftSink&#34;,
    &#34;name&#34;: &#34;RedshiftSinkConnector_0&#34;,
    &#34;input.data.format&#34;: &#34;AVRO&#34;,
    &#34;kafka.auth.mode&#34;: &#34;KAFKA_API_KEY&#34;,
    &#34;kafka.api.key&#34;: &#34;&lt;dd_your_api_key&gt;&#34;,
    &#34;kafka.api.secret&#34;: &#34;&lt;add_your_api_secret_key&gt;&#34;,
    &#34;topics&#34;: &#34;ratings-enriched&#34;,
    &#34;aws.redshift.domain&#34;: &#34;&lt;add_your_redshift_cluster_endpoint&gt;&#34;,
    &#34;aws.redshift.port&#34;: &#34;5439&#34;,
    &#34;aws.redshift.user&#34;: &#34;&lt;add_the_username_you_created_earlier&gt;&#34;,
    &#34;aws.redshift.password&#34;: &#34;&lt;add_the_corresponding_password&gt;&#34;,
    &#34;aws.redshift.database&#34;: &#34;&lt;name_of_the_database_you_created_earlier&gt;&#34;,
    &#34;auto.create&#34;: &#34;true&#34;,
    &#34;auto.evolve&#34;: &#34;true&#34;,
    &#34;tasks.max&#34;: &#34;1&#34;,
    &#34;transforms&#34;: &#34;Transform,Transform2 &#34;,
    &#34;transforms.Transform.type&#34;: &#34;org.apache.kafka.connect.transforms.Cast$Value&#34;,
    &#34;transforms.Transform.spec&#34;: &#34;DOB:string&#34;,
    &#34;transforms.Transform2.type&#34;: &#34;org.apache.kafka.connect.transforms.MaskField$Value&#34;,
    &#34;transforms.Transform2.fields&#34;: &#34;DOB&#34;,
    &#34;transforms.Transform2.replacement&#34;: &#34;&lt;xxxx-xx-xx&gt;&#34;
  }
}
</code></pre><ol start="4">
<li>In this lab, we decided to mask customer&rsquo;s date of birth before sinking the stream to Redshift. We are leverage Single Message Transforms (SMT) to achieve this goal. Since date of birth is of type <code>DATE</code> and we want to replace it with a string pattern, we will achieve our goal in a 2 step process. First, we will cast the date of birth from <code>DATE</code> to <code>String</code>, then we will replace that <code>String</code> value with a pattern we have pre-defined.</li>
</ol>
<blockquote>
<p>For more information on Single Message Transforms (SMT) refer to our <a href="https://docs.confluent.io/cloud/current/connectors/single-message-transforms.html">documentation</a> or watch the series by Robin Moffatt, staff developer advocate at Confluent <a href="https://www.youtube.com/watch?v=3Gj_SoyuTYk&amp;list=PL5T99fPsK7pq7LiaaL-S6b7wQqzxyjgya&amp;ab_channel=RobinMoffatt">here</a>.</p>
</blockquote>
<ol start="5">
<li>Click on <strong>Next</strong>.</li>
<li>Before launching the connector, you will be brought to the summary page. Once you have reviewed the configs and everything looks good, select <strong>Launch</strong>.</li>
<li>This should return you to the main Connectors landing page. Wait for your newly created connector to change status from <strong>Provisioning</strong> to <strong>Running</strong>.</li>
<li>The instructor will show you how to query the Redshift database and verify the data exist.</li>
</ol>
<hr>
<h2 id="a-namestep14astep-11-connect-s3-sink-to-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 11: Connect S3 sink to Confluent Cloud</h2>
<ol>
<li>For this use case we only want to store the <code>ratings_live</code> stream in S3 and not the customers&rsquo; information.</li>
<li>Use the left handside menu and navigate to <strong>Data Integration</strong> and go to <strong>Connectors</strong>. Click on <strong>+Add connector</strong>. Search for <strong>S3</strong> and click on the S3 Sink icon.</li>
<li>Enter the following configuration details. The remaining fields can be left blank.</li>
</ol>
<pre tabindex="0"><code>{
  &#34;name&#34;: &#34;S3_SINKConnector_0&#34;,
  &#34;config&#34;: {
    &#34;topics&#34;: pksqlc-***RATINGS_LIVE&#34;,
    &#34;input.data.format&#34;: &#34;AVRO&#34;,
    &#34;connector.class&#34;: &#34;S3_SINK&#34;,
    &#34;name&#34;: &#34;S3_SINKConnector_0&#34;,
    &#34;kafka.auth.mode&#34;: &#34;KAFKA_API_KEY&#34;,
    &#34;kafka.api.key&#34;: &#34;&lt;dd_your_api_key&gt;&#34;,
    &#34;kafka.api.secret&#34;: &#34;&lt;add_your_api_secret_key&gt;&#34;,
    &#34;aws.access.key.id&#34;: &#34;&lt;add_access_key_for_confluent-s3-demo-user&gt;&#34;,
    &#34;aws.secret.access.key&#34;: &#34;&lt;add_secret_access_key_for_confluent-s3-demo-user&gt;&#34;,
    &#34;s3.bucket.name&#34;: &#34;confluent-bucket-demo&#34;,
    &#34;output.data.format&#34;: &#34;JSON&#34;,
    &#34;time.interval&#34;: &#34;HOURLY&#34;,
    &#34;flush.size&#34;: &#34;1000&#34;,
    &#34;tasks.max&#34;: &#34;1&#34;
  }
}
</code></pre><ol start="4">
<li>The instructor will show you how to verify data exists in S3.</li>
</ol>
<hr>
<h2 id="a-namestep15astep-12-connect-dynamodb-sink-to-confluent-cloud"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 12: Connect DynamoDB sink to Confluent Cloud</h2>
<ol>
<li>For this use case, we will be streaming the <strong>users</strong> topic to DynamoDB database.</li>
<li>We decided to use <code>userid</code> as the <strong>hash key</strong> and <code>regionid</code> as the <strong>sort key</strong> in DynamoDB.</li>
<li>Additionally, we will use Single Message Transforms (SMT) to convert the timestamp to <code>String</code>.</li>
<li>Enter the following configuration details. The remaining fields can be left blank.</li>
</ol>
<pre tabindex="0"><code>{
  &#34;name&#34;: &#34;DynamoDbSinkConnector_0&#34;,
  &#34;config&#34;: {
    &#34;topics&#34;: &#34;users&#34;,
    &#34;input.data.format&#34;: &#34;AVRO&#34;,
    &#34;connector.class&#34;: &#34;DynamoDbSink&#34;,
    &#34;name&#34;: &#34;DynamoDbSinkConnector_0&#34;,
    &#34;kafka.auth.mode&#34;: &#34;KAFKA_API_KEY&#34;,
    &#34;kafka.api.key&#34;: &#34;&lt;dd_your_api_key&gt;&#34;,
    &#34;kafka.api.secret&#34;: &#34;&lt;add_your_api_secret_key&gt;&#34;,
    &#34;aws.access.key.id&#34;: &#34;&lt;add_access_key_for_confluent-dynamodb-demo-user&gt;&#34;,
    &#34;aws.secret.access.key&#34;: &#34;&lt;add_secret_access_key_for_confluent-dynamodb-demo-user&gt;&#34;,
    &#34;aws.dynamodb.pk.hash&#34;: &#34;value.userid&#34;,
    &#34;aws.dynamodb.pk.sort&#34;: &#34;value.regionid&#34;,
    &#34;table.name.format&#34;: &#34;confluent-${topic}&#34;,
    &#34;tasks.max&#34;: &#34;1&#34;,
    &#34;transforms&#34;: &#34;timestamp &#34;,
    &#34;transforms.timestamp.type&#34;: &#34;org.apache.kafka.connect.transforms.TimestampConverter$Value&#34;,
    &#34;transforms.timestamp.target.type&#34;: &#34;string&#34;,
    &#34;transforms.timestamp.field&#34;: &#34;registertime&#34;,
    &#34;transforms.timestamp.format&#34;: &#34;yyyy-MM-dd&#34;
  }
}
</code></pre><ol start="5">
<li>The instructor will show you how to verify data exists in DynamoDB table.</li>
</ol>
<hr>
<h2 id="a-namestep16astep-13-clean-up-resources"><!-- raw HTML omitted --><!-- raw HTML omitted -->Step 13: Clean up resources</h2>
<p>Deleting the resources you created during this lab will prevent you from incurring additional charges.</p>
<ol>
<li>The first item to delete is the ksqlDB application. Select the <strong>Delete</strong> button under <strong>Actions</strong> and enter the Application Name to confirm the deletion.</li>
<li>Delete the all source and sink connectors by navigating to <strong>Connectors</strong> in the navigation panel, clicking your connector name, then clicking the trash can icon in the upper right and entering the connector name to confirm the deletion.</li>
<li>Delete the Cluster by going to the <strong>Settings</strong> tab and then selecting <strong>Delete cluster</strong></li>
<li>Delete the Environment by expanding right hand menu and going to <strong>Environments</strong> tab and then clicking on <strong>Delete</strong> for the associated Environment you would like to delete</li>
<li>Go to <a href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a> and delete Redshift cluster, DynamoDB table, and S3 bucket. Additionally, you can delete IAM policy and users you created for this lab.</li>
</ol>
<hr>
<h2 id="a-namestep17aconfluent-resources-and-further-testing"><!-- raw HTML omitted --><!-- raw HTML omitted -->Confluent Resources and Further Testing</h2>
<p>Here are some links to check out if you are interested in further testing:</p>
<ul>
<li>
<p>Confluent Cloud <a href="https://docs.confluent.io/cloud/current/client-apps/cloud-basics.html">Basics</a></p>
</li>
<li>
<p><a href="https://docs.confluent.io/cloud/current/get-started/index.html">Quickstart</a> with Confluent Cloud</p>
</li>
<li>
<p>Confluent Cloud ksqlDB <a href="https://docs.confluent.io/cloud/current/get-started/ksql.html">Quickstart</a></p>
</li>
<li>
<p>Confluent Developer <a href="https://developer.confluent.io/">website</a></p>
</li>
</ul>


<footer class="footline">
	
</footer>

        
        </div>
        

      </div>

    <div id="navigation">
        
        

        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
            
        

        


	 
	 
		
			<a class="nav nav-prev" href="https://confluentinc.github.io/live-labs/aws/test/" title="Test"> <i class="fa fa-chevron-left"></i></a>
		
		
			<a class="nav nav-next" href="https://confluentinc.github.io/live-labs/aws/test/" title="Test" style="margin-right: 0px;"><i class="fa fa-chevron-right"></i></a>
		
	
    </div>

    </section>

    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="https://confluentinc.github.io/live-labs/js/clipboard.min.js?1646074555"></script>
    <script src="https://confluentinc.github.io/live-labs/js/perfect-scrollbar.min.js?1646074555"></script>
    <script src="https://confluentinc.github.io/live-labs/js/perfect-scrollbar.jquery.min.js?1646074555"></script>
    <script src="https://confluentinc.github.io/live-labs/js/jquery.sticky.js?1646074555"></script>
    <script src="https://confluentinc.github.io/live-labs/js/featherlight.min.js?1646074555"></script>
    <script src="https://confluentinc.github.io/live-labs/js/highlight.pack.js?1646074555"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="https://confluentinc.github.io/live-labs/js/modernizr.custom-3.6.0.js?1646074555"></script>
    <script src="https://confluentinc.github.io/live-labs/js/learn.js?1646074555"></script>
    <script src="https://confluentinc.github.io/live-labs/js/hugo-learn.js?1646074555"></script>
    
        
            <script src="https://confluentinc.github.io/live-labs/mermaid/mermaid.js?1646074555"></script>
        
        <script>
            mermaid.initialize({ startOnLoad: true });
        </script>
    
    

  </body>
</html>

