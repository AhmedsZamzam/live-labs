[
{
	"uri": "https://chuck-confluent.github.io/live-labs/gcp/",
	"title": "Gcp",
	"tags": [],
	"description": "",
	"content": "Chapter GCP Do Stuff Lorem Ipsum.\n"
},
{
	"uri": "https://chuck-confluent.github.io/live-labs/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "AWS Build An End-to-End Streaming Data Pipeline with Confluent Cloud For this lab, we have two fictional companies.\n An airline company: stores customer information in a MySQL database. It also has a website that customers can submit feedback in real time.   The analytics team decided to use AWS Redshift, a Cloud Data Warehouse. They want to be able to react to customers feedback as they become availabe. For example if a customer with Platinum club status had a bad experience, they want to reach out to them and sort things out. This team doesn\u0026rsquo;t want to go to two different sources to get their data, they want the data to become available to them in a format and location they decided is the right choice for them. The AI team wants to use real world data to train and test their AI models. They don\u0026rsquo;t want to go and find this data, so we are providing the customer rating data to them in AWS S3, which is a great solution to store large amounts of data for a long time.  A media company: recently their userbase grew significantly and their database is struggling to keep up. They concluded that AWS DynamoDB, a highly scalable NoSQL database, is the right choice for them, so they are migrating their users' information to DynamoDB.  To keep things simple, we will utilize Datagen Source Connector to generate both ratings and users data ourseleves. Additionally, we will use MySQL CDC Source Connecter, AWS Redshift, S3, and DynamoDB Sink fully-managed connectors.\n"
},
{
	"uri": "https://chuck-confluent.github.io/live-labs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Confluent Live Labs "
},
{
	"uri": "https://chuck-confluent.github.io/live-labs/gcp/cloudstorage-bigquery-bigtable/",
	"title": "CloudStorage Bigquery Bigtable",
	"tags": [],
	"description": "",
	"content": "For this lab, we have two fictional companies.\n An airline company: stores customer information in a MySQL database. It also has a website that customers can submit feedback in real time.   The analytics team decided to use GCP BigQuery, a Cloud Data Warehouse. They want to be able to react to customers feedback as they become availabe. For example if a customer with Platinum club status had a bad experience, they want to reach out to them and sort things out. This team doesn\u0026rsquo;t want to go to two different sources to get their data, they want the data to become available to them in a format and location they decided is the right choice for them. The AI team wants to use real world data to train and test their AI models. They don\u0026rsquo;t want to go and find this data, so we are providing the customer rating data to them in GCP Cloud Storage, which is a great solution to store large amounts of data for a long time.  A media company: recently their userbase grew significantly and their database is struggling to keep up. They concluded that GCP BigTable, a highly scalable NoSQL database, is the right choice for them, so they are migrating their users' information to BigTable.  To keep things simple, we will utilize Datagen Source Connector to generate both ratings and users data ourseleves. Additionally, we will use MySQL CDC Source Connecter, GCP BigQuery, Cloud Storage, and BigTable fully-managed connectors.\n Agenda  Log into Confluent Cloud Create an environment and cluster Create an API key pair Enable Schema registery Create a ksqlDB application Create \u0026ldquo;ratings\u0026rdquo; topic Create a Datagen Source connector Create customers topic Create a MySQL CDC Source connector Create \u0026ldquo;users\u0026rdquo; topic Create a Datagen Source connector Create GCP services Enrich data streams with ksqlDB Connect BigQuery sink to Confluent Cloud Connect Cloud Storage sink to Confluent Cloud Connect BigTable sink to Confluent Cloud Clean up resources   Architecture Diagram This lab will be utilizing two fully-managed source connectors (Datagen and MySQL CDC) and three fully-managed sink connectors (GCP BigQuery, Cloud Storage, BigTable).\nFictionAir FictionMedia  Prerequisites Sign up for Confluent Cloud Account   Sign up for a Confluent Cloud account here.\n  Once you have signed up and logged in, click on the menu icon at the upper right hand corner, click on “Billing \u0026amp; payment”, then enter payment details under “Payment details \u0026amp; contacts”.\n   Note: You will create resources during this lab that will incur costs. When you sign up for a Confluent Cloud account, you will get free credits to use in Confluent Cloud. This will cover the cost of resources created during the lab. More details on the specifics can be found here.\n Test Network Connectivity  Ports 443 and 9092 need to be open to the public internet for outbound traffic. To check, try accessing the following from your web browser:   portquiz.net:443 portquiz.net:9092  Sign up for GCP account  In order to complete this lab, you need to have a GCP account. Sign up for an account here.   Step 1: Log into Confluent Cloud  First, access Confluent Cloud sign-in by navigating here. When provided with the username and password prompts, fill in your credentials.  Note: If you\u0026rsquo;re logging in for the first time you will see a wizard that will walk you through the some tutorials. Minimize this as you will walk through these steps in this guide.\n    Step 2: Create an environment and cluster An environment contains Confluent clusters and its deployed components such as Connect, ksqlDB, and Schema Registry. You have the ability to create different environments based on your company\u0026rsquo;s requirements. Confluent has seen companies use environments to separate Development/Testing, Pre-Production, and Production clusters.\n  Click + Add environment.\n Note: There is a default environment ready in your account upon account creation. You can use this default environment for the purpose of this lab if you do not wish to create an additional environment.\n  Specify a meaningful name for your environment and then click Create.  Note: It will take a few minutes to assign the resources to make this new environment available for use.\n     Now that you have an environment, let\u0026rsquo;s create a cluster. Select Create Cluster.\n Note: Confluent Cloud clusters are available in 3 types: Basic, Standard, and Dedicated. Basic is intended for development use cases so you should use that for this lab. Basic clusters only support single zone availability. Standard and Dedicated clusters are intended for production use and support Multi-zone deployments. If you’re interested in learning more about the different types of clusters and their associated features and limits, refer to this documentation.\n   Choose the Basic cluster type.\n  Click Begin Configuration.\n  Choose GCP as your Cloud Provider and your preferred Region.\n Note: We recommend you choose Las Vegas (west4) as the region for the purpose of this lab.\n   Specify a meaningful Cluster Name and then review the associated Configuration \u0026amp; Cost, Usage Limits, and Uptime SLA before clicking Launch Cluster.\n     Step 3: Create an API key pair  Select API keys on the navigation menu. If this is your first API key within your cluster, click Create key. If you have set up API keys in your cluster in the past and already have an existing API key, click + Add key. Select Global Access, then click Next. Save your API key and secret - you will need these during the lab. After creating and saving the API key, you will see this API key in the Confluent Cloud UI in the API keys tab. If you don’t see the API key populate right away, refresh the browser.   Step 4: Enable Schema Registery  On the navigation menu, select Schema Registery. Click Set up on my own. Choose GCP as the cloud provider and a supported Region Click on Enable Schema Registry.   Hands-on Lab You have successfully completed the prep work. You can stop at this point and complete the remaining steps during the live session Step 1: Create a ksqlDB application  At Confluent we developed ksqlDB, the database purpose-built for stream processing applications. ksqlDB is built on top of Kafka Streams, powerful Java library for enriching, transforming, and processing real-time streams of data. Having Kafka Streams at its core means ksqlDB is built on well-designed and easily understood layers of abstractions. So now, beginners and experts alike can easily unlock and fully leverage the power of Kafka in a fun and accessible way.\n  On the navigation menu, select ksqlDB. Click on Create cluster myself. Choose Global access for the access level and hit Continue. Pick a name or leave the name as is. Select 1 as the cluster size. Hit Launch Cluster!.   Step 2: Create \u0026ldquo;ratings\u0026rdquo; topic  On the navigation menu, select Topics.   Click Create topic on my own or if you already created a topic, click on the + Add topic button on the top right side of the table.\n Type ratings as the Topic name and hit Create with defaults.   Step 3: Create a Datagen Source connector  Confluent offers 120+ pre-built connectors, enabling you to modernize your entire data architecture even faster. These connectors also provide you peace-of-mind with enterprise-grade security, reliability, compatibility, and support.\n  On the navigation menu, select Data Integration and then Connectors and + Add connector. In the search bar search for Datagen and select the Datagen Source which is a fully-managed connector that we will use to generate sample data with it. Use the following parameters to configure your connector  { \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;DatagenSource\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;kafka.topic\u0026quot;: \u0026quot;ratings\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;quickstart\u0026quot;: \u0026quot;RATINGS\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } }  Step 4: Create customers topic  On the navigation menu, select Topics.   Click Create topic on my own or if you already created a topic, click on the + Add topic button on the top right side of the table.\n Type mysql.demo.CUSTOMERS_INFO as the Topic name. The name of the topic is crucial so make sure you use the exact name and capitalization. Click on Show advanced settings and under Storage → Cleanup policy → Compact and Retention time → Indefinite and then click on Create.   Step 5: Create a MySQL CDC Source connector  On the navigation menu, select Data Integration and then Connectors and + Add connector. In the search bar search for MySQL CDC and select the MySQL CDC Source which is a fully-managed source connector. Use the following parameters to configure your connector  { \u0026quot;name\u0026quot;: \u0026quot;MySqlCdcSourceConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;MySqlCdcSource\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MySqlCdcSourceConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;database.hostname\u0026quot;: \u0026quot;\u0026lt;will_be_given_during_lab\u0026gt;\u0026quot;, \u0026quot;database.port\u0026quot;: \u0026quot;3306\u0026quot;, \u0026quot;database.user\u0026quot;: \u0026quot;\u0026lt;will_be_given_during_lab\u0026gt;\u0026quot;, \u0026quot;database.password\u0026quot;: \u0026quot;\u0026lt;will_be_given_during_lab\u0026gt;\u0026quot;, \u0026quot;database.server.name\u0026quot;: \u0026quot;mysql\u0026quot;, \u0026quot;database.ssl.mode\u0026quot;: \u0026quot;preferred\u0026quot;, \u0026quot;snapshot.mode\u0026quot;: \u0026quot;when_needed\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;after.state.only\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } }  Step 6: Create \u0026ldquo;users\u0026rdquo; topic  On the navigation menu, select Topics.   Click Create topic on my own or if you already created a topic, click on the + Add topic button on the top right side of the table.\n Type users as the Topic name and hit Create with defaults.   Step 7: Create a Datagen Source connector  On the navigation menu, select Data Integration and then Connectors and + Add connector. In the search bar search for Datagen and select the Datagen Source which is a fully-managed connector. Use the following parameters to configure your connector  { \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_1\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;DatagenSource\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;kafka.topic\u0026quot;: \u0026quot;users\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;quickstart\u0026quot;: \u0026quot;USERS\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } }  Step 8: Create GCP services BigQuery  Navigate to https://console.cloud.google.com and log into your account. Use the search bar and search for BigQuery   You should have a valid project in Google Cloud in order to complete this lab.\n Use the 3 dots next to your project name and click on Create a new dataset with following configuration.  Dataset ID: confluent_bigquery_demo Data location: same as your Confluent Cloud Cluster (in this lab us-west-4). Click on CREATE DATASET.   For detailed instructions refer to our documentation.\n Cloud Storage  Create a Cloud Storage bucket with the following configuration and leave the remaining fields as default.  Name: confluent-bucket-demo Location type: Region. Pick the same region as your Confluent Cloud cluster (In this lab we use us-west-4). Storage class: Standard Click on CREATE.   For detailed instructions refer to our documentation.\n BigTable  Create a BigTable instance with the following configurations and leave the remaining fields as default.  Instance name: confluent-bigtable-demo Instance ID: \u0026lt;auto_generated\u0026gt; Storage type: SSD Location: Region. Pick the same region as your Confluent Cloud cluster (In this lab we use us-west-4). Click on CREATE.   For detailed instructions refer to our documentation\n Service Accounts   You need to create a GCP Service Account so the Confluent Connectors can access GCP resources. For the purpose of this lab, we will create 1 Service Account and assign 3 roles to it. However, you should always follow best practices for production workloads.\n  Use the search bar and search for IAM \u0026amp; Admin and click on the result.\n  Expand the left hand side menu and go to Service Accounts and click on the Create Service Account and enter the following\n  Service account name: confluent-demo Description: Account to be used during Confluent Cloud Live Lab Role: BigQuery Admin, Bigtable Administrator, Storage Admin  Click Grant and then Done.\n  Once the account is created, click on the 3 dots in Actions columns and hit Manage Keys.\n  Click on Add Key and then on Create a new key.\n  Select JSON as key type. Keep this key somewhere safe as you will need it in later steps. The key resembles the example below:\n  { \u0026quot;type\u0026quot;: \u0026quot;service_account\u0026quot;, \u0026quot;project_id\u0026quot;: \u0026quot;confluent-842583\u0026quot;, \u0026quot;private_key_id\u0026quot;: \u0026quot;...omitted...\u0026quot;, \u0026quot;private_key\u0026quot;: \u0026quot;-----BEGIN PRIVATE ...omitted... =\\n-----END PRIVATE KEY-----\\n\u0026quot;, \u0026quot;client_email\u0026quot;: \u0026quot;confluent2@confluent-842583.iam.gserviceaccount.com\u0026quot;, \u0026quot;client_id\u0026quot;: \u0026quot;...omitted...\u0026quot;, \u0026quot;auth_uri\u0026quot;: \u0026quot;https://accounts.google.com/oauth2/auth\u0026quot;, \u0026quot;token_uri\u0026quot;: \u0026quot;https://oauth2.googleapis.com/token\u0026quot;, \u0026quot;auth_provider_x509_cert_url\u0026quot;: \u0026quot;https://www.googleapis.com/oauth2/certs\u0026quot;, \u0026quot;client_x509_cert_url\u0026quot;: \u0026quot;https://www.googleapis.com/robot/metadata/confluent2%40confluent-842583.iam.gserviceaccount.com\u0026quot; }  Step 9: Enrich data streams with ksqlDB Now that you have data flowing through Confluent, you can now easily build stream processing applications using ksqlDB. You are able to continuously transform, enrich, join, and aggregate your data using simple SQL syntax. You can gain value from your data directly from Confluent in real-time. Also, ksqlDB is a fully managed service within Confluent Cloud with a 99.9% uptime SLA. You can now focus on developing services and building your data pipeline while letting Confluent manage your resources for you.\nWith ksqlDB, you have the ability to leverage streams and tables from your topics in Confluent. A stream in ksqlDB is a topic with a schema and it records the history of what has happened in the world as a sequence of events.\n Navigate to confluent.cloud Use the left handside menu and go to the ksqlDB application you created at the beginning of the lab.   You can interact with ksqlDB through the Editor. You can create a stream by using the CREATE STREAM statement and a table using the CREATE TABLE statement. If you’re interested in learning more about ksqlDB and the differences between streams and tables, I recommend reading these two blogs here and here or watch ksqlDB 101 course on Confluent Developer webiste.\n To write streaming queries against topics, you will need to register the topics with ksqlDB as a stream and/or table.\nCreate a ksqlDB stream from ratings topic.  CREATE STREAM RATINGS_OG WITH (KAFKA_TOPIC=\u0026#39;ratings\u0026#39;, VALUE_FORMAT=\u0026#39;AVRO\u0026#39;); Change auto.offset.reset to Earliest and see what\u0026rsquo;s inside the RATINGS_OG stream by running the following query.  SELECT * FROM RATINGS_OG EMIT CHANGES; Create a new stream that doesn\u0026rsquo;t include messages from test channel.  CREATE STREAM RATINGS_LIVE AS SELECT * FROM RATINGS_OG WHERE LCASE(CHANNEL) NOT LIKE \u0026#39;%test%\u0026#39; EMIT CHANGES; See what\u0026rsquo;s inside RATINGS_LIVE stream by running the following query.  SELECT * FROM RATINGS_LIVE EMIT CHANGES;  Stop the running query by clicking on Stop.\n  Create a stream from customers topic.\n  CREATE STREAM CUSTOMERS_INFORMATION WITH (KAFKA_TOPIC =\u0026#39;mysql.demo.CUSTOMERS_INFO\u0026#39;, KEY_FORMAT =\u0026#39;JSON\u0026#39;, VALUE_FORMAT=\u0026#39;AVRO\u0026#39;); Create customers table based on customers_information stream you just created.  CREATE TABLE CUSTOMERS WITH (FORMAT=\u0026#39;AVRO\u0026#39;) AS SELECT id AS customer_id, LATEST_BY_OFFSET(first_name) AS first_name, LATEST_BY_OFFSET(last_name) AS last_name, LATEST_BY_OFFSET(dob) AS dob, LATEST_BY_OFFSET(email) AS email, LATEST_BY_OFFSET(gender) AS gender, LATEST_BY_OFFSET(club_status) AS club_status FROM CUSTOMERS_INFORMATION GROUP BY id; Check to see what\u0026rsquo;s inside the customers table by running the following query.  SELECT * FROM CUSTOMERS;  Now that we have a stream of ratings data and customer information, we can perform a join query to enrich our data stream.\n  Create a new stream by running the following statement.\n  CREATE STREAM RATINGS_WITH_CUSTOMER_DATA WITH (KAFKA_TOPIC=\u0026#39;ratings-enriched\u0026#39;) AS SELECT C.CUSTOMER_ID, C.FIRST_NAME + \u0026#39; \u0026#39; + C.LAST_NAME AS FULL_NAME, C.DOB, C.GENDER, C.CLUB_STATUS, C.EMAIL, R.RATING_ID, R.MESSAGE, R.STARS, R.CHANNEL, TIMESTAMPTOSTRING(R.ROWTIME,\u0026#39;yyyy-MM-dd\u0026#39;\u0026#39;T\u0026#39;\u0026#39;HH:mm:ss.SSSZ\u0026#39;) AS RATING_TS FROM RATINGS_LIVE R INNER JOIN CUSTOMERS C ON R.USER_ID = C.CUSTOMER_ID EMIT CHANGES; Change the auto.offset.reset to Earliest and see what\u0026rsquo;s inside the newly created stream by running the following command.  SELECT * FROM RATINGS_WITH_CUSTOMER_DATA EMIT CHANGES; Stop the running query by clicking on Stop.   Step 10: Connect BigQuery sink to Confluent Cloud  The next step is to sink data from Confluent Cloud into BigQuery using the fully-managed BigQuery Sink connector. The connector will continuosly run and send real time data into BigQuery. First, you will create the connector that will automatically create a BigQuery table and populate that table with the data from the ratings-enriched topic within Confluent Cloud. From the Confluent Cloud UI, click on the Data Integration tab on the navigation menu and select +Add connector. Search and click on the BigQuery Sink icon. Enter the following configuration details. The remaining fields can be left blank.  { \u0026quot;name\u0026quot;: \u0026quot;BigQuerySinkConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;topics\u0026quot;: \u0026quot;ratings-enriched\u0026quot;, \u0026quot;input.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;BigQuerySink\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BigQuerySinkConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;keyfile\u0026quot;: \u0026quot;\u0026lt;add_the_JSON_key_created_earlier\u0026gt;\u0026quot;, \u0026quot;project\u0026quot;: \u0026quot;\u0026lt;add_your_gcp_project_id\u0026gt;\u0026quot;, \u0026quot;datasets\u0026quot;: \u0026quot;confluent_bigquery_demo\u0026quot;, \u0026quot;auto.create.tables\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;auto.update.schemas\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;transforms\u0026quot;: \u0026quot;Transform,Transform3 \u0026quot;, \u0026quot;transforms.Transform.type\u0026quot;: \u0026quot;org.apache.kafka.connect.transforms.Cast$Value\u0026quot;, \u0026quot;transforms.Transform.spec\u0026quot;: \u0026quot;DOB:string\u0026quot;, \u0026quot;transforms.Transform3.type\u0026quot;: \u0026quot;org.apache.kafka.connect.transforms.MaskField$Value\u0026quot;, \u0026quot;transforms.Transform3.fields\u0026quot;: \u0026quot;DOB\u0026quot;, \u0026quot;transforms.Transform3.replacement\u0026quot;: \u0026quot;\u0026lt;xxxx-xx-xx\u0026gt;\u0026quot; } } In this lab, we decided to mask customer\u0026rsquo;s date of birth before sinking the stream to BigQuery. We are leveraging Single Message Transforms (SMT) to achieve this goal. Since date of birth is of type DATE and we want to replace it with a string pattern, we will achieve our goal in a 2 step process. First, we will cast the date of birth from DATE to String, then we will replace that String value with a pattern we have pre-defined.   For more information on Single Message Transforms (SMT) refer to our documentation or watch the series by Robin Moffatt, staff developer advocate at Confluent here.\n Click on Next. Before launching the connector, you will be brought to the summary page. Once you have reviewed the configs and everything looks good, select Launch. This should return you to the main Connectors landing page. Wait for your newly created connector to change status from Provisioning to Running. The instructor will show you how to query the BigQuery database and verify the data exist.   Step 11: Connect Cloud Storage sink to Confluent Cloud  For this use case we only want to store the ratings_live stream in Cloud Storage and not the customers' information. Use the left handside menu and navigate to Data Integration and go to Connectors. Click on +Add connector. Search for Google Cloud Storage Sink and click on the Google Cloud Storage Sink icon. Enter the following configuration details. The remaining fields can be left blank.  { \u0026quot;name\u0026quot;: \u0026quot;GcsSinkConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;topics\u0026quot;: \u0026quot;pksqlc**-RATINGS_LIVE\u0026quot;, \u0026quot;input.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;GcsSink\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;GcsSinkConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;gcs.credentials.config\u0026quot;: \u0026quot;\u0026lt;add_the_JSON_key_created_earlier\u0026gt;”, \u0026quot;gcs.bucket.name\u0026quot;: \u0026quot;confluent-bucket-demo\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;JSON\u0026quot;, \u0026quot;time.interval\u0026quot;: \u0026quot;HOURLY\u0026quot;, \u0026quot;flush.size\u0026quot;: \u0026quot;1000\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } } The instructor will show you how to verify data exists in Storage Sink.   Step 12: Connect BigTable sink to Confluent Cloud  For this use case, we will be streaming the users topic to BigTable database. We decided to concatenate userid and regionid fields to make the rowkey in BigTable Additionally, we will use Single Message Transforms (SMT) to convert the timestamp to String. Enter the following configuration details. The remaining fields can be left blank.  { \u0026quot;name\u0026quot;: \u0026quot;BigTableSinkConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;topics\u0026quot;: \u0026quot;users\u0026quot;, \u0026quot;input.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;input.key.format\u0026quot;: \u0026quot;STRING\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;BigTableSink\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BigTableSinkConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;gcp.bigtable.credentials.json\u0026quot;: \u0026quot;\u0026lt;add_the_JSON_key_created_earlier\u0026gt;\u0026quot;, \u0026quot;gcp.bigtable.project.id\u0026quot;: \u0026quot;\u0026lt;add_your_gcp_project_id\u0026gt;\u0026quot;, \u0026quot;gcp.bigtable.instance.id\u0026quot;: \u0026quot;confluent-bigtable-demo\u0026quot;, \u0026quot;insert.mode\u0026quot;: \u0026quot;UPSERT\u0026quot;, \u0026quot;table.name.format\u0026quot;: \u0026quot;confluent-${topic}\u0026quot;, \u0026quot;bigtable.row.key.definition\u0026quot;: \u0026quot;userid,regionid\u0026quot;, \u0026quot;bigtable.row.key.delimiter\u0026quot;: \u0026quot;#\u0026quot;, \u0026quot;auto.create.tables\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;transforms\u0026quot;: \u0026quot;Transform \u0026quot;, \u0026quot;transforms.Transform.type\u0026quot;: \u0026quot;org.apache.kafka.connect.transforms.TimestampConverter$Value\u0026quot;, \u0026quot;transforms.Transform.target.type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;transforms.Transform.field\u0026quot;: \u0026quot;registertime\u0026quot;, \u0026quot;transforms.Transform.format\u0026quot;: \u0026quot;yyyy-MM-dd\u0026quot; } } The instructor will show you how to verify data exists in BigTable table.   Step 13: Clean up resources Deleting the resources you created during this lab will prevent you from incurring additional charges.\n The first item to delete is the ksqlDB application. Select the Delete button under Actions and enter the Application Name to confirm the deletion. Delete the all source and sink connectors by navigating to Connectors in the navigation panel, clicking your connector name, then clicking the trash can icon in the upper right and entering the connector name to confirm the deletion. Delete the Cluster by going to the Settings tab and then selecting Delete cluster Delete the Environment by expanding right hand menu and going to Environments tab and then clicking on Delete for the associated Environment you would like to delete Go to https://console.cloud.google.com and delete BigQuery dataset, BigTable table, and Storage Sink bucket. Additionally, you can delete Service Account User and credential file you created for this lab.   Confluent Resources and Further Testing Here are some links to check out if you are interested in further testing:\n  Confluent Cloud Basics\n  Quickstart with Confluent Cloud\n  Confluent Cloud ksqlDB Quickstart\n  Confluent Developer website\n  "
},
{
	"uri": "https://chuck-confluent.github.io/live-labs/aws/test/",
	"title": "Test",
	"tags": [],
	"description": "",
	"content": "Hello hello\n"
},
{
	"uri": "https://chuck-confluent.github.io/live-labs/aws/dynamodb-redshift-s3/",
	"title": "Build An End-to-End Streaming Data Pipeline with Confluent Cloud",
	"tags": [],
	"description": "",
	"content": " Agenda  Log into Confluent Cloud Create an environment and cluster Create an API key pair Enable Schema registery Create a ksqlDB application Create \u0026ldquo;ratings\u0026rdquo; topic Create a Datagen Source connector Create customers topic Create a MySQL CDC Source connector Create \u0026ldquo;users\u0026rdquo; topic Create a Datagen Source connector Create AWS services Enrich data streams with ksqlDB Connect Redshift sink to Confluent Cloud Connect S3 sink to Confluent Cloud Connect DynamoDB sink to Confluent Cloud Clean up resources   Architecture Diagram This lab will be utilizing two fully-managed source connectors (Datagen and MySQL CDC) and three fully-managed sink connectors (AWS Redshift, S3, and DynamoDB).\nFictionAir FictionMedia  Prerequisites Sign up for Confluent Cloud Account   Sign up for a Confluent Cloud account here.\n  Once you have signed up and logged in, click on the menu icon at the upper right hand corner, click on “Billing \u0026amp; payment”, then enter payment details under “Payment details \u0026amp; contacts”.\n   Note: You will create resources during this lab that will incur costs. When you sign up for a Confluent Cloud account, you will get free credits to use in Confluent Cloud. This will cover the cost of resources created during the lab. More details on the specifics can be found here.\n Test Network Connectivity  Ports 443 and 9092 need to be open to the public internet for outbound traffic. To check, try accessing the following from your web browser:   portquiz.net:443 portquiz.net:9092  Sign up for AWS account  In order to complete this lab, you need to have an AWS account that has root level permissions. Sign up for an AWS account here.   Step 1: Log into Confluent Cloud  First, access Confluent Cloud sign-in by navigating here. When provided with the username and password prompts, fill in your credentials.  Note: If you\u0026rsquo;re logging in for the first time you will see a wizard that will walk you through the some tutorials. Minimize this as you will walk through these steps in this guide.\n    Step 2: Create an environment and cluster An environment contains Confluent clusters and its deployed components such as Connect, ksqlDB, and Schema Registry. You have the ability to create different environments based on your company\u0026rsquo;s requirements. Confluent has seen companies use environments to separate Development/Testing, Pre-Production, and Production clusters.\n  Click + Add environment.\n Note: There is a default environment ready in your account upon account creation. You can use this default environment for the purpose of this lab if you do not wish to create an additional environment.\n  Specify a meaningful name for your environment and then click Create.  Note: It will take a few minutes to assign the resources to make this new environment available for use.\n     Now that you have an environment, let\u0026rsquo;s create a cluster. Select Create Cluster.\n Note: Confluent Cloud clusters are available in 3 types: Basic, Standard, and Dedicated. Basic is intended for development use cases so you should use that for this lab. Basic clusters only support single zone availability. Standard and Dedicated clusters are intended for production use and support Multi-zone deployments. If you’re interested in learning more about the different types of clusters and their associated features and limits, refer to this documentation.\n   Choose the Basic cluster type.\n  Click Begin Configuration.\n  Choose AWS as your Cloud Provider and your preferred Region.\n Note: AWS account with root permissions is required as your Cloud Provider since you will be utilizing Redshift, S3, and DynamoDB in this lab. We recommend you choose Oregon (west2) as the region.\n   Specify a meaningful Cluster Name and then review the associated Configuration \u0026amp; Cost, Usage Limits, and Uptime SLA before clicking Launch Cluster.\n     Step 3: Create an API key pair  Select API keys on the navigation menu. If this is your first API key within your cluster, click Create key. If you have set up API keys in your cluster in the past and already have an existing API key, click + Add key. Select Global Access, then click Next. Save your API key and secret - you will need these during the lab. After creating and saving the API key, you will see this API key in the Confluent Cloud UI in the API keys tab. If you don’t see the API key populate right away, refresh the browser.   Step 4: Enable Schema Registery  On the navigation menu, select Schema Registery. Click Set up on my own. Choose AWS as the cloud provider and a supported Region Click on Enable Schema Registry.   Hands-on Lab You have successfully completed the prep work. You can stop at this point and complete the remaining steps during the live session Step 1: Create a ksqlDB application  At Confluent we developed ksqlDB, the database purpose-built for stream processing applications. ksqlDB is built on top of Kafka Streams, powerful Java library for enriching, transforming, and processing real-time streams of data. Having Kafka Streams at its core means ksqlDB is built on well-designed and easily understood layers of abstractions. So now, beginners and experts alike can easily unlock and fully leverage the power of Kafka in a fun and accessible way.\n  On the navigation menu, select ksqlDB. Click on Create cluster myself. Choose Global access for the access level and hit Continue. Pick a name or leave the name as is. Select 1 as the cluster size. Hit Launch Cluster!.   Step 2: Create \u0026ldquo;ratings\u0026rdquo; topic  On the navigation menu, select Topics.   Click Create topic on my own or if you already created a topic, click on the + Add topic button on the top right side of the table.\n Type ratings as the Topic name and hit Create with defaults.   Step 3: Create a Datagen Source connector  Confluent offers 120+ pre-built connectors, enabling you to modernize your entire data architecture even faster. These connectors also provide you peace-of-mind with enterprise-grade security, reliability, compatibility, and support.\n  On the navigation menu, select Data Integration and then Connectors and + Add connector. In the search bar search for Datagen and select the Datagen Source which is a fully-managed connector that we will use to generate sample data with it. Use the following parameters to configure your connector  { \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;DatagenSource\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;kafka.topic\u0026quot;: \u0026quot;ratings\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;quickstart\u0026quot;: \u0026quot;RATINGS\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } }  Step 4: Create customers topic  On the navigation menu, select Topics.   Click Create topic on my own or if you already created a topic, click on the + Add topic button on the top right side of the table.\n Type mysql.demo.CUSTOMERS_INFO as the Topic name. The name of the topic is crucial so make sure you use the exact name and capitalization. Click on Show advanced settings and under Storage → Cleanup policy → Compact and Retention time → Indefinite and then click on Create.   Step 5: Create a MySQL CDC Source connector  On the navigation menu, select Data Integration and then Connectors and + Add connector. In the search bar search for MySQL CDC and select the MySQL CDC Source which is a fully-managed source connector. Use the following parameters to configure your connector  { \u0026quot;name\u0026quot;: \u0026quot;MySqlCdcSourceConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;MySqlCdcSource\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MySqlCdcSourceConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;database.hostname\u0026quot;: \u0026quot;\u0026lt;will_be_given_during_lab\u0026gt;\u0026quot;, \u0026quot;database.port\u0026quot;: \u0026quot;3306\u0026quot;, \u0026quot;database.user\u0026quot;: \u0026quot;\u0026lt;will_be_given_during_lab\u0026gt;\u0026quot;, \u0026quot;database.password\u0026quot;: \u0026quot;\u0026lt;will_be_given_during_lab\u0026gt;\u0026quot;, \u0026quot;database.server.name\u0026quot;: \u0026quot;mysql\u0026quot;, \u0026quot;database.ssl.mode\u0026quot;: \u0026quot;preferred\u0026quot;, \u0026quot;snapshot.mode\u0026quot;: \u0026quot;when_needed\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;after.state.only\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } }  Step 6: Create \u0026ldquo;users\u0026rdquo; topic  On the navigation menu, select Topics.   Click Create topic on my own or if you already created a topic, click on the + Add topic button on the top right side of the table.\n Type users as the Topic name and hit Create with defaults.   Step 7: Create a Datagen Source connector  On the navigation menu, select Data Integration and then Connectors and + Add connector. In the search bar search for Datagen and select the Datagen Source which is a fully-managed connector. Use the following parameters to configure your connector  { \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_1\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;DatagenSource\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;DatagenSourceConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;add_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;kafka.topic\u0026quot;: \u0026quot;users\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;quickstart\u0026quot;: \u0026quot;USERS\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } }  Step 8: Create AWS services  Navigate to https://aws.amazon.com/console/ and log into your account.   Note: you will need root level permissions in order to complete this lab.\n Redshift  Create a Redshift cluster and save the Admin user name and Admin user password since you will need it in later steps.   The Redshift cluster has to be in same same region as your Confluent Cloud cluster.\n Under Additional configuration disable defaults. Make the cluster publicly accessible under Network and security → Publicly accessible → Enable. Using the search bar navigate to VPC -\u0026gt; Security Groups -\u0026gt; Inbound Rules and add two new rules for TCP protocol  Type: Redshift Port range: 5430 Source: 0.0.0.0/0 Type: Redshift Port range: 5430 Source: ::/0 Navigate back to your Redshift cluster and reboot it to ensure the right security policies are applied. Once the cluster is in Available state use the left handside menu and open Query Editor v2 to create a database and a user and give the appropriate permissions.  CREATE DATABASE \u0026lt;DB_NAME\u0026gt;; CREATE USER \u0026lt;DB_USER\u0026gt; PASSWORD \u0026#39;\u0026lt;DB_PASSWORD\u0026gt;\u0026#39;; GRANT USAGE ON SCHEMA public TO \u0026lt;DB_USER\u0026gt;; GRANT CREATE ON SCHEMA public TO \u0026lt;DB_USER\u0026gt;; GRANT SELECT ON ALL TABLES IN SCHEMA public TO \u0026lt;DB_USER\u0026gt;; GRANT ALL ON SCHEMA public TO \u0026lt;DB_USER\u0026gt;; GRANT CREATE ON DATABASE \u0026lt;DB_NAME\u0026gt; TO \u0026lt;DB_USER\u0026gt;;  For detailed instructions refer to our documentation\n S3  Create an S3 bucket and name it confluent-bucket-demo.   The S3 has to be in same same region as your Confluent Cloud cluster.\n IAM Policy and User  Create an IAM Policy with the following configuration and name it confluent-s3-demo-policy.  { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:ListAllMyBuckets\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::*\u0026quot; }, { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;s3:GetBucketLocation\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::confluent-bucket-demo\u0026quot; }, { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:AbortMultipartUpload\u0026quot;, \u0026quot;s3:ListMultipartUploadParts\u0026quot;, \u0026quot;s3:ListBucketMultipartUploads\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::confluent-bucket-demo\u0026quot;, \u0026quot;arn:aws:s3:::confluent-bucket-demo/*\u0026quot; ] } ] }  Create an IAM User and name it confluent-s3-demo-user. and attach the above policy to it.\n  Attach confluent-s3-demo-policy policy to confluent-s3-demo-user user.\n  Create a key pair for confluent-s3-demo-user user and download the file to use it in later steps.\n   For detailed instructions refer to our documentation\n Create a new IAM Policy with the following configurations and name it confluent-dynamodb-demo-policy.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;\u0026lt;optional-identifier\u0026gt;\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;dynamodb:CreateTable\u0026quot;, \u0026quot;dynamodb:BatchWriteItem\u0026quot;, \u0026quot;dynamodb:Scan\u0026quot;, \u0026quot;dynamodb:DescribeTable\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  Create another IAM User and name it confluent-dynamodb-demo-user. This one will be used for DynamoDB.\n  Attach confluent-dynamodb-demo-policy policy to confluent-dynamodb-demo-user user.\n  Create a key pair for confluent-dynamodb-demo-user user and download the file to use it in later steps.\n   For detailed instructions refer to our documentation\n  Step 9: Enrich data streams with ksqlDB Now that you have data flowing through Confluent, you can now easily build stream processing applications using ksqlDB. You are able to continuously transform, enrich, join, and aggregate your data using simple SQL syntax. You can gain value from your data directly from Confluent in real-time. Also, ksqlDB is a fully managed service within Confluent Cloud with a 99.9% uptime SLA. You can now focus on developing services and building your data pipeline while letting Confluent manage your resources for you.\nWith ksqlDB, you have the ability to leverage streams and tables from your topics in Confluent. A stream in ksqlDB is a topic with a schema and it records the history of what has happened in the world as a sequence of events.\n Navigate to confluent.cloud Use the left handside menu and go to the ksqlDB application you created at the beginning of the lab.   You can interact with ksqlDB through the Editor. You can create a stream by using the CREATE STREAM statement and a table using the CREATE TABLE statement. If you’re interested in learning more about ksqlDB and the differences between streams and tables, I recommend reading these two blogs here and here or watch ksqlDB 101 course on Confluent Developer webiste.\n To write streaming queries against topics, you will need to register the topics with ksqlDB as a stream and/or table.\nCreate a ksqlDB stream from ratings topic.  CREATE STREAM RATINGS_OG WITH (KAFKA_TOPIC=\u0026#39;ratings\u0026#39;, VALUE_FORMAT=\u0026#39;AVRO\u0026#39;); Change auto.offset.reset to Earliest and see what\u0026rsquo;s inside the RATINGS_OG stream by running the following query.  SELECT * FROM RATINGS_OG EMIT CHANGES; Create a new stream that doesn\u0026rsquo;t include messages from test channel.  CREATE STREAM RATINGS_LIVE AS SELECT * FROM RATINGS_OG WHERE LCASE(CHANNEL) NOT LIKE \u0026#39;%test%\u0026#39; EMIT CHANGES; See what\u0026rsquo;s inside RATINGS_LIVE stream by running the following query.  SELECT * FROM RATINGS_LIVE EMIT CHANGES;  Stop the running query by clicking on Stop.\n  Create a stream from customers topic.\n  CREATE STREAM CUSTOMERS_INFORMATION WITH (KAFKA_TOPIC =\u0026#39;mysql.demo.CUSTOMERS_INFO\u0026#39;, KEY_FORMAT =\u0026#39;JSON\u0026#39;, VALUE_FORMAT=\u0026#39;AVRO\u0026#39;); Create customers table based on customers_information stream you just created.  CREATE TABLE CUSTOMERS WITH (FORMAT=\u0026#39;AVRO\u0026#39;) AS SELECT id AS customer_id, LATEST_BY_OFFSET(first_name) AS first_name, LATEST_BY_OFFSET(last_name) AS last_name, LATEST_BY_OFFSET(dob) AS dob, LATEST_BY_OFFSET(email) AS email, LATEST_BY_OFFSET(gender) AS gender, LATEST_BY_OFFSET(club_status) AS club_status FROM CUSTOMERS_INFORMATION GROUP BY id; Check to see what\u0026rsquo;s inside the customers table by running the following query.  SELECT * FROM CUSTOMERS;  Now that we have a stream of ratings data and customer information, we can perform a join query to enrich our data stream.\n  Create a new stream by running the following statement.\n  CREATE STREAM RATINGS_WITH_CUSTOMER_DATA WITH (KAFKA_TOPIC=\u0026#39;ratings-enriched\u0026#39;) AS SELECT C.CUSTOMER_ID, C.FIRST_NAME + \u0026#39; \u0026#39; + C.LAST_NAME AS FULL_NAME, C.DOB, C.GENDER, C.CLUB_STATUS, C.EMAIL, R.RATING_ID, R.MESSAGE, R.STARS, R.CHANNEL, TIMESTAMPTOSTRING(R.ROWTIME,\u0026#39;yyyy-MM-dd\u0026#39;\u0026#39;T\u0026#39;\u0026#39;HH:mm:ss.SSSZ\u0026#39;) AS RATING_TS FROM RATINGS_LIVE R INNER JOIN CUSTOMERS C ON R.USER_ID = C.CUSTOMER_ID EMIT CHANGES; Change the auto.offset.reset to Earliest and see what\u0026rsquo;s inside the newly created stream by running the following command.  SELECT * FROM RATINGS_WITH_CUSTOMER_DATA EMIT CHANGES; Stop the running query by clicking on Stop.   Step 10: Connect Redshift sink to Confluent Cloud  The next step is to sink data from Confluent Cloud into Redshift using the fully-managed Redshift Sink connector. The connector will continuosly run and send real time data into Redshift. First, you will create the connector that will automatically create a Redshift table and populate that table with the data from the ratings-enriched topic within Confluent Cloud. From the Confluent Cloud UI, click on the Data Integration tab on the navigation menu and select +Add connector. Search and click on the Redshift Sink icon. Enter the following configuration details. The remaining fields can be left blank.  { \u0026quot;name\u0026quot;: \u0026quot;RedshiftSinkConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;RedshiftSink\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;RedshiftSinkConnector_0\u0026quot;, \u0026quot;input.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;dd_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;topics\u0026quot;: \u0026quot;ratings-enriched\u0026quot;, \u0026quot;aws.redshift.domain\u0026quot;: \u0026quot;\u0026lt;add_your_redshift_cluster_endpoint\u0026gt;\u0026quot;, \u0026quot;aws.redshift.port\u0026quot;: \u0026quot;5439\u0026quot;, \u0026quot;aws.redshift.user\u0026quot;: \u0026quot;\u0026lt;add_the_username_you_created_earlier\u0026gt;\u0026quot;, \u0026quot;aws.redshift.password\u0026quot;: \u0026quot;\u0026lt;add_the_corresponding_password\u0026gt;\u0026quot;, \u0026quot;aws.redshift.database\u0026quot;: \u0026quot;\u0026lt;name_of_the_database_you_created_earlier\u0026gt;\u0026quot;, \u0026quot;auto.create\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;auto.evolve\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;transforms\u0026quot;: \u0026quot;Transform,Transform2 \u0026quot;, \u0026quot;transforms.Transform.type\u0026quot;: \u0026quot;org.apache.kafka.connect.transforms.Cast$Value\u0026quot;, \u0026quot;transforms.Transform.spec\u0026quot;: \u0026quot;DOB:string\u0026quot;, \u0026quot;transforms.Transform2.type\u0026quot;: \u0026quot;org.apache.kafka.connect.transforms.MaskField$Value\u0026quot;, \u0026quot;transforms.Transform2.fields\u0026quot;: \u0026quot;DOB\u0026quot;, \u0026quot;transforms.Transform2.replacement\u0026quot;: \u0026quot;\u0026lt;xxxx-xx-xx\u0026gt;\u0026quot; } } In this lab, we decided to mask customer\u0026rsquo;s date of birth before sinking the stream to Redshift. We are leverage Single Message Transforms (SMT) to achieve this goal. Since date of birth is of type DATE and we want to replace it with a string pattern, we will achieve our goal in a 2 step process. First, we will cast the date of birth from DATE to String, then we will replace that String value with a pattern we have pre-defined.   For more information on Single Message Transforms (SMT) refer to our documentation or watch the series by Robin Moffatt, staff developer advocate at Confluent here.\n Click on Next. Before launching the connector, you will be brought to the summary page. Once you have reviewed the configs and everything looks good, select Launch. This should return you to the main Connectors landing page. Wait for your newly created connector to change status from Provisioning to Running. The instructor will show you how to query the Redshift database and verify the data exist.   Step 11: Connect S3 sink to Confluent Cloud  For this use case we only want to store the ratings_live stream in S3 and not the customers' information. Use the left handside menu and navigate to Data Integration and go to Connectors. Click on +Add connector. Search for S3 and click on the S3 Sink icon. Enter the following configuration details. The remaining fields can be left blank.  { \u0026quot;name\u0026quot;: \u0026quot;S3_SINKConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;topics\u0026quot;: pksqlc-***RATINGS_LIVE\u0026quot;, \u0026quot;input.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;S3_SINK\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;S3_SINKConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;dd_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;aws.access.key.id\u0026quot;: \u0026quot;\u0026lt;add_access_key_for_confluent-s3-demo-user\u0026gt;\u0026quot;, \u0026quot;aws.secret.access.key\u0026quot;: \u0026quot;\u0026lt;add_secret_access_key_for_confluent-s3-demo-user\u0026gt;\u0026quot;, \u0026quot;s3.bucket.name\u0026quot;: \u0026quot;confluent-bucket-demo\u0026quot;, \u0026quot;output.data.format\u0026quot;: \u0026quot;JSON\u0026quot;, \u0026quot;time.interval\u0026quot;: \u0026quot;HOURLY\u0026quot;, \u0026quot;flush.size\u0026quot;: \u0026quot;1000\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot; } } The instructor will show you how to verify data exists in S3.   Step 12: Connect DynamoDB sink to Confluent Cloud  For this use case, we will be streaming the users topic to DynamoDB database. We decided to use userid as the hash key and regionid as the sort key in DynamoDB. Additionally, we will use Single Message Transforms (SMT) to convert the timestamp to String. Enter the following configuration details. The remaining fields can be left blank.  { \u0026quot;name\u0026quot;: \u0026quot;DynamoDbSinkConnector_0\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;topics\u0026quot;: \u0026quot;users\u0026quot;, \u0026quot;input.data.format\u0026quot;: \u0026quot;AVRO\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;DynamoDbSink\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;DynamoDbSinkConnector_0\u0026quot;, \u0026quot;kafka.auth.mode\u0026quot;: \u0026quot;KAFKA_API_KEY\u0026quot;, \u0026quot;kafka.api.key\u0026quot;: \u0026quot;\u0026lt;dd_your_api_key\u0026gt;\u0026quot;, \u0026quot;kafka.api.secret\u0026quot;: \u0026quot;\u0026lt;add_your_api_secret_key\u0026gt;\u0026quot;, \u0026quot;aws.access.key.id\u0026quot;: \u0026quot;\u0026lt;add_access_key_for_confluent-dynamodb-demo-user\u0026gt;\u0026quot;, \u0026quot;aws.secret.access.key\u0026quot;: \u0026quot;\u0026lt;add_secret_access_key_for_confluent-dynamodb-demo-user\u0026gt;\u0026quot;, \u0026quot;aws.dynamodb.pk.hash\u0026quot;: \u0026quot;value.userid\u0026quot;, \u0026quot;aws.dynamodb.pk.sort\u0026quot;: \u0026quot;value.regionid\u0026quot;, \u0026quot;table.name.format\u0026quot;: \u0026quot;confluent-${topic}\u0026quot;, \u0026quot;tasks.max\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;transforms\u0026quot;: \u0026quot;timestamp \u0026quot;, \u0026quot;transforms.timestamp.type\u0026quot;: \u0026quot;org.apache.kafka.connect.transforms.TimestampConverter$Value\u0026quot;, \u0026quot;transforms.timestamp.target.type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;transforms.timestamp.field\u0026quot;: \u0026quot;registertime\u0026quot;, \u0026quot;transforms.timestamp.format\u0026quot;: \u0026quot;yyyy-MM-dd\u0026quot; } } The instructor will show you how to verify data exists in DynamoDB table.   Step 13: Clean up resources Deleting the resources you created during this lab will prevent you from incurring additional charges.\n The first item to delete is the ksqlDB application. Select the Delete button under Actions and enter the Application Name to confirm the deletion. Delete the all source and sink connectors by navigating to Connectors in the navigation panel, clicking your connector name, then clicking the trash can icon in the upper right and entering the connector name to confirm the deletion. Delete the Cluster by going to the Settings tab and then selecting Delete cluster Delete the Environment by expanding right hand menu and going to Environments tab and then clicking on Delete for the associated Environment you would like to delete Go to https://aws.amazon.com/console/ and delete Redshift cluster, DynamoDB table, and S3 bucket. Additionally, you can delete IAM policy and users you created for this lab.   Confluent Resources and Further Testing Here are some links to check out if you are interested in further testing:\n  Confluent Cloud Basics\n  Quickstart with Confluent Cloud\n  Confluent Cloud ksqlDB Quickstart\n  Confluent Developer website\n  "
},
{
	"uri": "https://chuck-confluent.github.io/live-labs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://chuck-confluent.github.io/live-labs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]